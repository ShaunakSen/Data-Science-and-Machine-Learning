{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA in Python\n",
    "\n",
    "[link](http://sebastianraschka.com/Articles/2014_pca_step_by_step.html)\n",
    "\n",
    "The main purposes of a principal component analysis are the analysis of data to identify patterns and finding patterns to reduce the dimensions of the dataset with minimal loss of information.\n",
    "\n",
    "Here, our desired outcome of the principal component analysis is to project a feature space (our dataset consisting of n d-dimensional samples) onto a smaller subspace that represents our data “well”. A possible application would be a pattern classification task, where we want to reduce the computational costs and the error of parameter estimation by reducing the number of dimensions of our feature space by extracting a subspace that describes our data “best”.\n",
    "\n",
    "**In PCA, we are interested to find the directions (components) that maximize the variance in our dataset**\n",
    "\n",
    "### PCA vs MDA\n",
    "\n",
    "Both Multiple Discriminant Analysis (MDA) and Principal Component Analysis (PCA) are linear transformation methods and closely related to each other. In PCA, we are interested to find the directions (components) that maximize the variance in our dataset, where in MDA, we are additionally interested to find the directions that maximize the separation (or discrimination) between different classes (for example, in pattern classification problems where our dataset consists of multiple classes. In contrast two PCA, which ignores the class labels).\n",
    "\n",
    "**In other words, via PCA, we are projecting the entire set of data (without class labels) onto a different subspace, and in MDA, we are trying to determine a suitable subspace to distinguish between patterns that belong to different classes. Or, roughly speaking in PCA we are trying to find the axes with maximum variances where the data is most spread (within a class, since PCA treats the whole data set as one class), and in MDA we are additionally maximizing the spread between classes.**\n",
    "\n",
    "\n",
    "### What is a good subspace?\n",
    "\n",
    "Let’s assume that our goal is to reduce the dimensions of a d-dimensional dataset by projecting it onto a (k)-dimensional subspace (where k<d). So, how do we know what size we should choose for k, and how do we know if we have a feature space that represents our data “well”?\n",
    "Later, we will compute eigenvectors (the components) from our data set and collect them in a so-called scatter-matrix (or alternatively calculate them from the covariance matrix). Each of those eigenvectors is associated with an eigenvalue, which tell us about the “length” or “magnitude” of the eigenvectors. If we observe that all the eigenvalues are of very similar magnitude, this is a good indicator that our data is already in a “good” subspace. Or if some of the eigenvalues are much much higher than others, we might be interested in keeping only those eigenvectors with the much larger eigenvalues, since they contain more information about our data distribution. Vice versa, eigenvalues that are close to 0 are less informative and we might consider in dropping those when we construct the new feature subspace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.49671415  1.52302986  1.57921282  0.54256004  0.24196227 -0.56228753\n",
      "  -0.90802408 -0.2257763  -0.54438272  0.37569802 -0.60170661 -1.05771093\n",
      "   0.2088636   0.19686124 -0.11564828 -0.71984421  0.34361829 -0.38508228\n",
      "   1.03099952 -0.30921238]\n",
      " [-0.1382643  -0.23415337  0.76743473 -0.46341769 -1.91328024 -1.01283112\n",
      "  -1.4123037   0.0675282   0.11092259 -0.60063869  1.85227818  0.82254491\n",
      "  -1.95967012  0.73846658 -0.3011037  -0.46063877 -1.76304016 -0.676922\n",
      "   0.93128012  0.33126343]\n",
      " [ 0.64768854 -0.23413696 -0.46947439 -0.46572975 -1.72491783  0.31424733\n",
      "   1.46564877 -1.42474819 -1.15099358 -0.29169375 -0.01349722 -1.22084365\n",
      "  -1.32818605  0.17136828 -1.47852199  1.05712223  0.32408397  0.61167629\n",
      "  -0.83921752  0.97554513]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42) # random seed for consistency\n",
    "\n",
    "# A reader pointed out that Python 2.7 would raise a\n",
    "# \"ValueError: object of too small depth for desired array\".\n",
    "# This can be avoided by choosing a smaller random seed, e.g. 1\n",
    "# or by completely omitting this line, since I just used the random seed for\n",
    "# consistency.\n",
    "\n",
    "mu_vec1 = np.array([0,0,0])\n",
    "cov_mat1 = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "\n",
    "# Draw random samples from a multivariate normal distribution \n",
    "\n",
    "\n",
    "\n",
    "class1_sample = np.random.multivariate_normal(mu_vec1, cov_mat1, 20).T\n",
    "\n",
    "\n",
    "print(lenclass1_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
