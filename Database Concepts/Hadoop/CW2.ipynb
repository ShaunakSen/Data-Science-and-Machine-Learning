{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2: Data Processing\n",
    "\n",
    "## Task 1\n",
    "This coursework will assess your understanding of using NoSQL to store and retrieve data.  You will perform operations on data from the Enron email dataset in a MongoDB database, and write a report detailing the suitability of different types of databases for data science applications.  You will be required to run code to answer the given questions in the Jupyter notebook provided, and write a report describing alternative approaches to using MongoDB.\n",
    "\n",
    "Download the JSON version of the Enron data (using the “Download as zip” to download the data file from http://edshare.soton.ac.uk/19548/, the file is about 380MB) and import into a collection called messages in a database called enron.  You do not need to set up any authentication.  In the Jupyter notebook provided, perform the following tasks, using the Python PyMongo library.\n",
    "\n",
    "Answers should be efficient in terms of speed.  Answers which are less efficient will not get full marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)\n",
    "Write a function which returns a MongoDB connection object to the \"messages\" collection. [4 points] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "get_collection",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_collection():\n",
    "    \"\"\"\n",
    "    Connects to the server, and returns a collection object\n",
    "    of the `messages` collection in the `enron` database\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)\n",
    "\n",
    "Write a function which returns the amount of emails in the messages collection in total. [4 points] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "get_amount_of_messages",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_amount_of_messages(collection):\n",
    "    \"\"\"\n",
    "    :param collection A PyMongo collection object\n",
    "    :return the amount of documents in the collection\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) \n",
    "\n",
    "Write a function which returns each person who was BCCed on an email.  Include each person only once, and display only their name according to the X-To header. [4 points] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "get_bcced_people",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_bcced_people(collection):\n",
    "    \"\"\"\n",
    "    :param collection A PyMongo collection object\n",
    "    :return the names of the people who have received an email by BCC\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)\n",
    "\n",
    "Write a function with parameter subject, which gets all emails in a thread with that parameter, and orders them by date (ascending). “An email thread is an email message that includes a running list of all the succeeding replies starting with the original email.”, check for detail descriptions at https://www.techopedia.com/definition/1503/email-thread [4 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "get_emails_in_thread",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_emails_in_thread(collection, subject):\n",
    "    \"\"\"\n",
    "    :param collection A PyMongo collection object\n",
    "    :return All emails in the thread with that subject\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)\n",
    "\n",
    "Write a function which returns the percentage of emails sent on a weekend (i.e., Saturday and Sunday) as a `float` between 0 and 1. [6 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "get_percentage_sent_on_weekend",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_percentage_sent_on_weekend(collection):\n",
    "    \"\"\"\n",
    "    :param collection A PyMongo collection object\n",
    "    :return A float between 0 and 1\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6)\n",
    "\n",
    "Write a function with parameter limit. The function should return for each email account: the number of emails sent, the number of emails received, and the total number of emails (sent and received). Use the following format: [{\"contact\": \"michael.simmons@enron.com\", \"from\": 42, \"to\": 92, \"total\": 134}] and the information contained in the To, From, and Cc headers. Sort the output in descending order by the total number of emails. Use the parameter limit to specify the number of results to be returned. If limit is null, the function should return all results. If limit is higher than null, the function should return the number of results specified as limit. limit cannot take negative values. [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "get_emails_between_contacts",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_emails_between_contacts(collection, limit):\n",
    "    \"\"\"\n",
    "    Shows the communications between contacts\n",
    "    Sort by the descending order of total emails using the To, From, and Cc headers.\n",
    "    :param `collection` A PyMongo collection object    \n",
    "    :param `limit` An integer specifying the amount to display, or\n",
    "    if null will display all outputs\n",
    "    :return A list of objects of the form:\n",
    "    [{\n",
    "        'contact': <<Another email address>>\n",
    "        'from': \n",
    "        'to': \n",
    "        'total': \n",
    "    },{.....}]\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7)\n",
    "Write a function to find out the number of senders who were also direct receivers. Direct receiver means the email is sent to the person directly, not via cc or bcc. [4 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_from_to_people(collection):\n",
    "    \"\"\"\n",
    "    :param collection A PyMongo collection object\n",
    "    :return the NUMBER of the people who have sent emails and received emails as direct receivers.\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8)\n",
    "Write a function with parameters start_date and end_date, which returns the number of email messages that have been sent between those specified dates, including start_date and end_date [4 points] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_emails_between_dates(collection, start_date, end_date):\n",
    "    \"\"\"\n",
    "    :param collection A PyMongo collection object\n",
    "    :return All emails between the specified start_date and end_date\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "This task will assess your ability to use the Hadoop Streaming API and MapReduce to process data. For each of the questions below, you are expected to write two python scripts, one for the Map phase and one for the Reduce phase. You are also expected to provide the correct parameters to the `hadoop` command to run the MapReduce process. Write down your answers in the specified cells below.\n",
    "\n",
    "To get started, you need to download and unzip the YouTube dataset (available at http://edshare.soton.ac.uk/19547/) onto the machine where you have Hadoop installed (this should be the virtual machine provided).\n",
    "\n",
    "To help you, `%%writefile` has been added to the top of the cells, automatically writing them to \"mapper.py\" and \"reducer.py\" respectively when the cells are run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) \n",
    "Using Youtube01-Psy.csv, find the hourly interval in which most spam was sent. The output should be in the form of a single key-value pair, where the value is a datetime at the start of the hour with the highest number of spam comments. [9 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7|11|2013|6\t1\n",
      "7|11|2013|12\t1\n",
      "19|1|2014|20\t1\n"
     ]
    }
   ],
   "source": [
    "# DEBUGGING SCRIPT FOR MAPPER\n",
    "\n",
    "dates = [\n",
    "'2013-11-07T06:20:48',\n",
    "'2013-11-07T12:37:15',\n",
    "'2014-01-19T04:27:18',\n",
    "'2014-01-19T08:55:53',\n",
    "'2014-01-19T20:31:10'\n",
    "]\n",
    "\n",
    "spam_class = [1,1,0,0,1]\n",
    "\n",
    "for x in range(len(dates)):\n",
    "    if spam_class[x] == 1:\n",
    "        date = dates[x].strip()\n",
    "        date_as_date = datetime.strptime(date, '%Y-%m-%dT%H:%M:%S')\n",
    "        day = date_as_date.date().day\n",
    "        month = date_as_date.date().month\n",
    "        year = date_as_date.date().year\n",
    "        hour = date_as_date.hour\n",
    "\n",
    "        print (str(day) + '|' + str(month) + '|' + str(year) + '|' + str(hour) + '\\t' + '1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [1,2,3]\n",
    "\n",
    "test = test[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "# MAPPER\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "lines = sys.stdin.readlines()\n",
    "\n",
    "\n",
    "csvreader = csv.reader(lines)\n",
    "\n",
    "dates = []\n",
    "\n",
    "spam_class = []\n",
    "\n",
    "input_for_reducer = []\n",
    "\n",
    "counter = 0\n",
    "for row in csvreader:\n",
    "    if counter > 0:\n",
    "        dates.append(row[2])\n",
    "        spam_class.append(row[4])\n",
    "    counter += 1\n",
    "    \n",
    "\n",
    "if (len(dates) != len(spam_class)):\n",
    "    print ('Unequal number of entries in Date and Class columns... Aborting...')\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "for x in range(len(dates)):\n",
    "    if spam_class[x] == '1':\n",
    "        date = dates[x].strip()\n",
    "        date_as_date = datetime.strptime(date, '%Y-%m-%dT%H:%M:%S')\n",
    "        day = date_as_date.date().day\n",
    "        month = date_as_date.date().month\n",
    "        year = date_as_date.date().year\n",
    "        hour = date_as_date.hour\n",
    "    \n",
    "        print (str(day) + '|' + str(month) + '|' + str(year) + '|' + str(hour) + '\\t' + '1')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dates in our input file are arranged such that the dates (at an hourly interval) occur in groups, we can perform the Reduce operation in linear time.\n",
    "\n",
    "It is observed in the data that the column 'Date' is indeed sorted in ascending order\n",
    "\n",
    "So the dates (at an hourly interval) are in groups\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hour_with_most_spam': '7|11|2013|12', 'value_of_max_spam_count': 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUGGING SCRIPT FOR REDUCER\n",
    "\n",
    "input_pairs = [\n",
    "    '7|11|2013|6\t1',\n",
    "    '7|11|2013|6\t1',\n",
    "    '7|11|2013|12\t1',\n",
    "    '7|11|2013|12\t1',\n",
    "    '7|11|2013|12\t1',\n",
    "    '19|1|2014|20\t1'\n",
    "]\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "date_count_dict = dict()\n",
    "\n",
    "final_dict = {\n",
    "    'hour_with_most_spam': None,\n",
    "    'value_of_max_spam_count': 0\n",
    "}\n",
    "\n",
    "\n",
    "for input_pair in input_pairs:\n",
    "    input_list = input_pair.split('\\t', 1)\n",
    "    if (len(input_list) != 2):\n",
    "        continue\n",
    "    \n",
    "    dates_list.append(input_list[0])\n",
    "        \n",
    "dates_list\n",
    "\n",
    "for date in dates_list:\n",
    "    if date in date_count_dict.keys():\n",
    "        date_count_dict[date] += 1\n",
    "    else:\n",
    "        date_count_dict[date] = 1\n",
    "        \n",
    "date_count_dict_sorted = sorted(date_count_dict.items(), key=lambda date_count_value: date_count_value[1], \n",
    "                                reverse=True)\n",
    "\n",
    "final_dict['hour_with_most_spam'] = date_count_dict_sorted[0][0]\n",
    "final_dict['value_of_max_spam_count'] = date_count_dict_sorted[0][1]\n",
    "\n",
    "final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "# REDUCER\n",
    "\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "input_pairs = sys.stdin.readlines()\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "date_count_dict = dict()\n",
    "\n",
    "final_dict = {\n",
    "    'hour_with_most_spam': None,\n",
    "    'value_of_max_spam_count': 0\n",
    "}\n",
    "\n",
    "\n",
    "for input_pair in input_pairs:\n",
    "    input_list = input_pair.split('\\t', 1)\n",
    "    if (len(input_list) != 2):\n",
    "        continue\n",
    "    \n",
    "    dates_list.append(input_list[0])\n",
    "        \n",
    "dates_list\n",
    "\n",
    "for date in dates_list:\n",
    "    if date in date_count_dict.keys():\n",
    "        date_count_dict[date] += 1\n",
    "    else:\n",
    "        date_count_dict[date] = 1\n",
    "        \n",
    "date_count_dict_sorted = sorted(date_count_dict.items(), key=lambda date_count_value: date_count_value[1], \n",
    "                                reverse=True)\n",
    "\n",
    "final_dict['hour_with_most_spam'] = date_count_dict_sorted[0][0]\n",
    "final_dict['value_of_max_spam_count'] = date_count_dict_sorted[0][1]\n",
    "\n",
    "\n",
    "for key, value in final_dict.items():\n",
    "    print (key + \"\\t\" + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "myList = [1,1,1,2,2,2,2,3,3]\n",
    "\n",
    "max_count = 1\n",
    "max_elem = myList[0]\n",
    "curr_count = 1\n",
    "\n",
    "for x in range(1, len(myList)):\n",
    "    if (myList[x] == myList[x-1]):\n",
    "        # same elem, inc counter \n",
    "        curr_count += 1\n",
    "    else:\n",
    "        # diff elem\n",
    "        if curr_count > max_count:\n",
    "            max_count = curr_count\n",
    "            max_elem = myList[x - 1]\n",
    "        curr_count = 1\n",
    "# last element check\n",
    "if curr_count > max_count:\n",
    "    max_count = curr_count\n",
    "    max_elem = myList[x - 1]\n",
    "\n",
    "    \n",
    "print (max_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_of_max_spam_count\t4\n",
      "hour_with_most_spam\t8|11|2014|10\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat ./Youtube01-Psy.csv | ./mapper.py  | ./reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop switched to standalone mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenJDK Server VM warning: You have loaded library /opt/hadoop-2.8.5/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.\n",
      "It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'.\n",
      "18/12/14 00:55:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "18/12/14 00:55:02 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "18/12/14 00:55:02 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "18/12/14 00:55:02 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "18/12/14 00:55:03 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "18/12/14 00:55:03 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "18/12/14 00:55:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1724741641_0001\n",
      "18/12/14 00:55:03 INFO mapred.LocalDistributedCacheManager: Localized file:/home/comp6235/Notebooks/CW2/mapper.py as file:/tmp/hadoop-comp6235/mapred/local/1544748903567/mapper.py\n",
      "18/12/14 00:55:04 INFO mapred.LocalDistributedCacheManager: Localized file:/home/comp6235/Notebooks/CW2/reducer.py as file:/tmp/hadoop-comp6235/mapred/local/1544748903568/reducer.py\n",
      "18/12/14 00:55:04 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "18/12/14 00:55:04 INFO mapreduce.Job: Running job: job_local1724741641_0001\n",
      "18/12/14 00:55:04 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "18/12/14 00:55:04 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "18/12/14 00:55:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/14 00:55:04 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/14 00:55:04 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "18/12/14 00:55:04 INFO mapred.LocalJobRunner: Starting task: attempt_local1724741641_0001_m_000000_0\n",
      "18/12/14 00:55:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/14 00:55:04 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/14 00:55:04 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/12/14 00:55:04 INFO mapred.MapTask: Processing split: file:/home/comp6235/Notebooks/CW2/Youtube01-Psy.csv:0+57438\n",
      "18/12/14 00:55:04 INFO mapred.MapTask: numReduceTasks: 1\n",
      "18/12/14 00:55:04 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "18/12/14 00:55:04 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "18/12/14 00:55:04 INFO mapred.MapTask: soft limit at 83886080\n",
      "18/12/14 00:55:04 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "18/12/14 00:55:04 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "18/12/14 00:55:04 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "18/12/14 00:55:04 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/CW2/././mapper.py]\n",
      "18/12/14 00:55:04 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "18/12/14 00:55:04 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "18/12/14 00:55:04 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "18/12/14 00:55:04 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "18/12/14 00:55:04 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "18/12/14 00:55:04 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "18/12/14 00:55:04 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "18/12/14 00:55:04 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "18/12/14 00:55:04 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "18/12/14 00:55:04 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "18/12/14 00:55:04 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "18/12/14 00:55:04 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "18/12/14 00:55:04 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/14 00:55:04 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/14 00:55:04 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/14 00:55:04 INFO streaming.PipeMapRed: Records R/W=351/1\n",
      "18/12/14 00:55:04 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/12/14 00:55:04 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/12/14 00:55:04 INFO mapred.LocalJobRunner: \n",
      "18/12/14 00:55:04 INFO mapred.MapTask: Starting flush of map output\n",
      "18/12/14 00:55:04 INFO mapred.MapTask: Spilling map output\n",
      "18/12/14 00:55:04 INFO mapred.MapTask: bufstart = 0; bufend = 2604; bufvoid = 104857600\n",
      "18/12/14 00:55:04 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213700(104854800); length = 697/6553600\n",
      "18/12/14 00:55:04 INFO mapred.MapTask: Finished spill 0\n",
      "18/12/14 00:55:04 INFO mapred.Task: Task:attempt_local1724741641_0001_m_000000_0 is done. And is in the process of committing\n",
      "18/12/14 00:55:04 INFO mapred.LocalJobRunner: Records R/W=351/1\n",
      "18/12/14 00:55:04 INFO mapred.Task: Task 'attempt_local1724741641_0001_m_000000_0' done.\n",
      "18/12/14 00:55:04 INFO mapred.Task: Final Counters for attempt_local1724741641_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=193294\n",
      "\t\tFILE: Number of bytes written=522297\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=351\n",
      "\t\tMap output records=175\n",
      "\t\tMap output bytes=2604\n",
      "\t\tMap output materialized bytes=2960\n",
      "\t\tInput split bytes=103\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=175\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=27\n",
      "\t\tTotal committed heap usage (bytes)=137433088\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=57438\n",
      "18/12/14 00:55:04 INFO mapred.LocalJobRunner: Finishing task: attempt_local1724741641_0001_m_000000_0\n",
      "18/12/14 00:55:04 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "18/12/14 00:55:04 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "18/12/14 00:55:04 INFO mapred.LocalJobRunner: Starting task: attempt_local1724741641_0001_r_000000_0\n",
      "18/12/14 00:55:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/14 00:55:04 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/14 00:55:04 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/12/14 00:55:04 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1279e20\n",
      "18/12/14 00:55:04 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "18/12/14 00:55:05 INFO reduce.EventFetcher: attempt_local1724741641_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "18/12/14 00:55:05 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1724741641_0001_m_000000_0 decomp: 2956 len: 2960 to MEMORY\n",
      "18/12/14 00:55:05 INFO reduce.InMemoryMapOutput: Read 2956 bytes from map-output for attempt_local1724741641_0001_m_000000_0\n",
      "18/12/14 00:55:05 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2956, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2956\n",
      "18/12/14 00:55:05 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "18/12/14 00:55:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "18/12/14 00:55:05 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "18/12/14 00:55:05 INFO mapred.Merger: Merging 1 sorted segments\n",
      "18/12/14 00:55:05 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2940 bytes\n",
      "18/12/14 00:55:05 INFO reduce.MergeManagerImpl: Merged 1 segments, 2956 bytes to disk to satisfy reduce memory limit\n",
      "18/12/14 00:55:05 INFO reduce.MergeManagerImpl: Merging 1 files, 2960 bytes from disk\n",
      "18/12/14 00:55:05 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "18/12/14 00:55:05 INFO mapred.Merger: Merging 1 sorted segments\n",
      "18/12/14 00:55:05 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2940 bytes\n",
      "18/12/14 00:55:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "18/12/14 00:55:05 INFO mapreduce.Job: Job job_local1724741641_0001 running in uber mode : false\n",
      "18/12/14 00:55:05 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/12/14 00:55:05 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/CW2/././reducer.py]\n",
      "18/12/14 00:55:05 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "18/12/14 00:55:05 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "18/12/14 00:55:05 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/14 00:55:05 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/14 00:55:05 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/14 00:55:05 INFO streaming.PipeMapRed: Records R/W=175/1\n",
      "18/12/14 00:55:05 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/12/14 00:55:05 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/12/14 00:55:05 INFO mapred.Task: Task:attempt_local1724741641_0001_r_000000_0 is done. And is in the process of committing\n",
      "18/12/14 00:55:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "18/12/14 00:55:05 INFO mapred.Task: Task attempt_local1724741641_0001_r_000000_0 is allowed to commit now\n",
      "18/12/14 00:55:05 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1724741641_0001_r_000000_0' to file:/home/comp6235/Notebooks/CW2/output1/_temporary/0/task_local1724741641_0001_r_000000\n",
      "18/12/14 00:55:05 INFO mapred.LocalJobRunner: Records R/W=175/1 > reduce\n",
      "18/12/14 00:55:05 INFO mapred.Task: Task 'attempt_local1724741641_0001_r_000000_0' done.\n",
      "18/12/14 00:55:05 INFO mapred.Task: Final Counters for attempt_local1724741641_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=199246\n",
      "\t\tFILE: Number of bytes written=525328\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=127\n",
      "\t\tReduce shuffle bytes=2960\n",
      "\t\tReduce input records=175\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=175\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=4\n",
      "\t\tTotal committed heap usage (bytes)=137433088\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=71\n",
      "18/12/14 00:55:05 INFO mapred.LocalJobRunner: Finishing task: attempt_local1724741641_0001_r_000000_0\n",
      "18/12/14 00:55:05 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "18/12/14 00:55:06 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/12/14 00:55:06 INFO mapreduce.Job: Job job_local1724741641_0001 completed successfully\n",
      "18/12/14 00:55:06 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=392540\n",
      "\t\tFILE: Number of bytes written=1047625\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=351\n",
      "\t\tMap output records=175\n",
      "\t\tMap output bytes=2604\n",
      "\t\tMap output materialized bytes=2960\n",
      "\t\tInput split bytes=103\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=127\n",
      "\t\tReduce shuffle bytes=2960\n",
      "\t\tReduce input records=175\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=350\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=31\n",
      "\t\tTotal committed heap usage (bytes)=274866176\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=57438\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=71\n",
      "18/12/14 00:55:06 INFO streaming.StreamJob: Output directory: output1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Clear output\n",
    "rm -rf output1\n",
    "\n",
    "# Make sure hadoop is in standalone mode\n",
    "hadoop-standalone-mode.sh\n",
    "\n",
    "# Main pipeline command\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "-files mapper.py,reducer.py \\\n",
    "-input Youtube01-Psy.csv \\\n",
    "-mapper ./mapper.py \\\n",
    "-reducer ./reducer.py \\\n",
    "-output output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenJDK Server VM warning: You have loaded library /opt/hadoop-2.8.5/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.\n",
      "It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'.\n",
      "18/12/11 16:33:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "18/12/11 16:33:32 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "18/12/11 16:33:32 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "18/12/11 16:33:32 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "18/12/11 16:33:33 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "18/12/11 16:33:33 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "18/12/11 16:33:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local647788955_0001\n",
      "18/12/11 16:33:33 INFO mapred.LocalDistributedCacheManager: Localized file:/home/comp6235/Notebooks/CW2/mapper.py as file:/tmp/hadoop-comp6235/mapred/local/1544546013513/mapper.py\n",
      "18/12/11 16:33:33 INFO mapred.LocalDistributedCacheManager: Localized file:/home/comp6235/Notebooks/CW2/reducer.py as file:/tmp/hadoop-comp6235/mapred/local/1544546013514/reducer.py\n",
      "18/12/11 16:33:34 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "18/12/11 16:33:34 INFO mapreduce.Job: Running job: job_local647788955_0001\n",
      "18/12/11 16:33:34 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "18/12/11 16:33:34 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "18/12/11 16:33:34 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/11 16:33:34 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/11 16:33:34 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "18/12/11 16:33:34 INFO mapred.LocalJobRunner: Starting task: attempt_local647788955_0001_m_000000_0\n",
      "18/12/11 16:33:34 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/11 16:33:34 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/11 16:33:34 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/12/11 16:33:34 INFO mapred.MapTask: Processing split: file:/home/comp6235/Notebooks/CW2/Youtube01-Psy.csv:0+57438\n",
      "18/12/11 16:33:34 INFO mapred.MapTask: numReduceTasks: 1\n",
      "18/12/11 16:33:34 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "18/12/11 16:33:34 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "18/12/11 16:33:34 INFO mapred.MapTask: soft limit at 83886080\n",
      "18/12/11 16:33:34 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "18/12/11 16:33:34 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "18/12/11 16:33:34 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "18/12/11 16:33:34 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/CW2/././mapper.py]\n",
      "18/12/11 16:33:34 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "18/12/11 16:33:34 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "18/12/11 16:33:34 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "18/12/11 16:33:34 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "18/12/11 16:33:34 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "18/12/11 16:33:34 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "18/12/11 16:33:34 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "18/12/11 16:33:34 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "18/12/11 16:33:34 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "18/12/11 16:33:34 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "18/12/11 16:33:34 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "18/12/11 16:33:34 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "18/12/11 16:33:34 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/11 16:33:34 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/11 16:33:34 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/11 16:33:34 INFO streaming.PipeMapRed: Records R/W=351/1\n",
      "18/12/11 16:33:34 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/12/11 16:33:34 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/12/11 16:33:34 INFO mapred.LocalJobRunner: \n",
      "18/12/11 16:33:34 INFO mapred.MapTask: Starting flush of map output\n",
      "18/12/11 16:33:34 INFO mapred.MapTask: Spilling map output\n",
      "18/12/11 16:33:34 INFO mapred.MapTask: bufstart = 0; bufend = 2604; bufvoid = 104857600\n",
      "18/12/11 16:33:34 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213700(104854800); length = 697/6553600\n",
      "18/12/11 16:33:34 INFO mapred.MapTask: Finished spill 0\n",
      "18/12/11 16:33:34 INFO mapred.Task: Task:attempt_local647788955_0001_m_000000_0 is done. And is in the process of committing\n",
      "18/12/11 16:33:34 INFO mapred.LocalJobRunner: Records R/W=351/1\n",
      "18/12/11 16:33:34 INFO mapred.Task: Task 'attempt_local647788955_0001_m_000000_0' done.\n",
      "18/12/11 16:33:34 INFO mapred.Task: Final Counters for attempt_local647788955_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=194013\n",
      "\t\tFILE: Number of bytes written=521202\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=351\n",
      "\t\tMap output records=175\n",
      "\t\tMap output bytes=2604\n",
      "\t\tMap output materialized bytes=2960\n",
      "\t\tInput split bytes=103\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=175\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=27\n",
      "\t\tTotal committed heap usage (bytes)=137433088\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=57438\n",
      "18/12/11 16:33:34 INFO mapred.LocalJobRunner: Finishing task: attempt_local647788955_0001_m_000000_0\n",
      "18/12/11 16:33:34 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "18/12/11 16:33:34 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "18/12/11 16:33:34 INFO mapred.LocalJobRunner: Starting task: attempt_local647788955_0001_r_000000_0\n",
      "18/12/11 16:33:34 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/11 16:33:34 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/11 16:33:34 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/12/11 16:33:34 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1e2b8f8\n",
      "18/12/11 16:33:34 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "18/12/11 16:33:34 INFO reduce.EventFetcher: attempt_local647788955_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "18/12/11 16:33:34 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local647788955_0001_m_000000_0 decomp: 2956 len: 2960 to MEMORY\n",
      "18/12/11 16:33:34 INFO reduce.InMemoryMapOutput: Read 2956 bytes from map-output for attempt_local647788955_0001_m_000000_0\n",
      "18/12/11 16:33:34 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2956, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2956\n",
      "18/12/11 16:33:34 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "18/12/11 16:33:34 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "18/12/11 16:33:34 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "18/12/11 16:33:34 INFO mapred.Merger: Merging 1 sorted segments\n",
      "18/12/11 16:33:34 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2940 bytes\n",
      "18/12/11 16:33:34 INFO reduce.MergeManagerImpl: Merged 1 segments, 2956 bytes to disk to satisfy reduce memory limit\n",
      "18/12/11 16:33:34 INFO reduce.MergeManagerImpl: Merging 1 files, 2960 bytes from disk\n",
      "18/12/11 16:33:34 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "18/12/11 16:33:34 INFO mapred.Merger: Merging 1 sorted segments\n",
      "18/12/11 16:33:34 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2940 bytes\n",
      "18/12/11 16:33:34 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "18/12/11 16:33:34 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/CW2/././reducer.py]\n",
      "18/12/11 16:33:34 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "18/12/11 16:33:34 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "18/12/11 16:33:35 INFO mapreduce.Job: Job job_local647788955_0001 running in uber mode : false\n",
      "18/12/11 16:33:35 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/12/11 16:33:35 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/11 16:33:35 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/11 16:33:35 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/11 16:33:35 INFO streaming.PipeMapRed: Records R/W=175/1\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/comp6235/Notebooks/CW2/././reducer.py\", line 68, in <module>\n",
      "    final_dict['hour_with_most_spam'] = max_elem\n",
      "NameError: name 'max_elem' is not defined\n",
      "18/12/11 16:33:35 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/12/11 16:33:35 INFO streaming.PipeMapRed: PipeMapRed failed!\n",
      "java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:346)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "18/12/11 16:33:35 INFO streaming.PipeMapRed: PipeMapRed failed!\n",
      "java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)\n",
      "\tat org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:251)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:346)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "18/12/11 16:33:35 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "18/12/11 16:33:35 WARN fs.FileUtil: Failed to delete file or dir [/home/comp6235/Notebooks/CW2/output/_temporary/0/_temporary/attempt_local647788955_0001_r_000000_0]: it still exists.\n",
      "18/12/11 16:33:35 WARN mapred.LocalJobRunner: job_local647788955_0001\n",
      "java.lang.Exception: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:489)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:556)\n",
      "Caused by: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)\n",
      "\tat org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:251)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:346)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "18/12/11 16:33:36 INFO mapreduce.Job: Job job_local647788955_0001 failed with state FAILED due to: NA\n",
      "18/12/11 16:33:36 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=194013\n",
      "\t\tFILE: Number of bytes written=521202\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=351\n",
      "\t\tMap output records=175\n",
      "\t\tMap output bytes=2604\n",
      "\t\tMap output materialized bytes=2960\n",
      "\t\tInput split bytes=103\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=0\n",
      "\t\tReduce shuffle bytes=2960\n",
      "\t\tReduce input records=0\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=175\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=27\n",
      "\t\tTotal committed heap usage (bytes)=137433088\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=57438\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "18/12/11 16:33:36 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Hadoop command to run the map reduce.\n",
    "\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "-files mapper.py,reducer.py   \\\n",
    "-input Youtube01-Psy.csv   \\\n",
    "-mapper  ./mapper.py \\\n",
    "-reducer ./reducer.py  \\\n",
    "-output output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Expected key-value output format:\n",
    "#hour_with_most_spam\t\"2013-11-10T10:00:00\"\n",
    "\n",
    "#Additional key-value pairs are acceptable, as long as the hour_with_most_spam pair is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) \n",
    "Find all comments associated with a username (the AUTHOR field). Return a JSON array of all comments associated with that username. (This should use the data from all 5 data files: Psy, KatyPerry, LMFAO, Eminem, Shakira) [11 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper1.py\n",
    "#!/usr/bin/env python\n",
    "#Answer for mapper.py\n",
    "\n",
    "# importing the libraries\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "\n",
    "def mapper_function(required_username):\n",
    "\n",
    "    # function that accepts an username as input \n",
    "\n",
    "    # counter keeps track of number of rows left, so that we can skip the first row (headers)\n",
    "    counter = 0\n",
    "    for row in csvreader:\n",
    "        if counter > 0:\n",
    "            usernames.append(row[1])\n",
    "            comments.append(row[3])\n",
    "        counter += 1\n",
    "\n",
    "    if (len(usernames) != len(comments)):\n",
    "        print ('Unequal number of entries in Author and Content... Aborting...')\n",
    "        sys.exit()\n",
    "\n",
    "    # pass the required username and the comments for that username to reducer stage\n",
    "    for x in range(len(usernames)):\n",
    "        if required_username == usernames[x]:\n",
    "            print (str(usernames[x]) + '\\t' + str(comments[x]))\n",
    "            \n",
    "\n",
    "\n",
    "lines = sys.stdin.readlines()\n",
    "\n",
    "# read from csv\n",
    "\n",
    "csvreader = csv.reader(lines)\n",
    "\n",
    "usernames = []\n",
    "\n",
    "comments = []\n",
    "\n",
    "\n",
    "# get username from command line argument\n",
    "\n",
    "required_username = str(sys.argv[1])\n",
    "\n",
    "mapper_function(required_username)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "#!/usr/bin/env python\n",
    "#Answer for reducer.py\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "final_dict = {\n",
    "    'username': None,\n",
    "    'comments': []\n",
    "}\n",
    "\n",
    "# get input from mapper job\n",
    "\n",
    "input_pairs = sys.stdin.readlines()\n",
    "\n",
    "\n",
    "\n",
    "for input_pair in input_pairs:\n",
    "    # split the tab separated input (username\\tcomment)\n",
    "    input_list = input_pair.split('\\t', 1)\n",
    "    if (len(input_list) != 2):\n",
    "        continue\n",
    "        \n",
    "    # append each comment\n",
    "    final_dict['comments'].append(input_list[1])\n",
    "    # set the username if it is not set\n",
    "    if final_dict['username'] is None:\n",
    "        final_dict['username'] = input_list[0]\n",
    "    \n",
    "\n",
    "# print out the output in desired form: username\\t[..comments..]\n",
    "print (final_dict.values()[0] + '\\t' + str(final_dict.values()[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini\t['Weeeeeeeeeeeeeee yes blaaaah file 2', 'Weeeeeeeeeeeeeee yes blaaaah file 2', 'Weeeeeeeeeeeeeee yes blaaaah', 'Weeeeeeeeeeeeeee yes blaaaah', 'Weeeeeeeeeeeeeee yes blaaaah file 3', 'Weeeeeeeeeeeeeee yes blaaaah file 3', 'Weeeeeeeeeeeeeee yes blaaaah file 4', 'Weeeeeeeeeeeeeee yes blaaaah file 4', 'Weeeeeeeeeeeeeee yes blaaaah file 5', 'Weeeeeeeeeeeeeee yes blaaaah file 5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "close failed in file object destructor:\n",
      "sys.excepthook is missing\n",
      "lost sys.stderr\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat ./test_files/Youtube02-KatyPerry.csv ./test_files/Youtube01-Psy.csv \\\n",
    "./test_files/Youtube03-LMFAO.csv ./test_files/Youtube04-Eminem.csv ./test_files/Youtube05-Shakira.csv | ./mapper1.py 'Mini' | ./reducer1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop switched to standalone mode.\n",
      "packageJobJar: [./mapper1.py] [] /tmp/streamjob6493429099037452166.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenJDK Server VM warning: You have loaded library /opt/hadoop-2.8.5/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.\n",
      "It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'.\n",
      "18/12/14 00:27:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "18/12/14 00:27:28 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "18/12/14 00:27:28 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "18/12/14 00:27:28 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "18/12/14 00:27:28 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "18/12/14 00:27:28 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "18/12/14 00:27:28 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "18/12/14 00:27:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1818403589_0001\n",
      "18/12/14 00:27:30 INFO mapred.LocalDistributedCacheManager: Localized file:/home/comp6235/Notebooks/CW2/mapper1.py as file:/tmp/hadoop-comp6235/mapred/local/1544747249524/mapper1.py\n",
      "18/12/14 00:27:30 INFO mapred.LocalDistributedCacheManager: Localized file:/home/comp6235/Notebooks/CW2/reducer1.py as file:/tmp/hadoop-comp6235/mapred/local/1544747249525/reducer1.py\n",
      "18/12/14 00:27:30 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "18/12/14 00:27:30 INFO mapreduce.Job: Running job: job_local1818403589_0001\n",
      "18/12/14 00:27:30 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "18/12/14 00:27:30 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "18/12/14 00:27:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/14 00:27:30 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/14 00:27:30 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "18/12/14 00:27:30 INFO mapred.LocalJobRunner: Starting task: attempt_local1818403589_0001_m_000000_0\n",
      "18/12/14 00:27:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/14 00:27:30 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/14 00:27:30 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: Processing split: file:/home/comp6235/Notebooks/CW2/test_files/Youtube03-LMFAO.csv:0+64794\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: numReduceTasks: 1\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: soft limit at 83886080\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "18/12/14 00:27:30 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/CW2/./mapper1.py, Mini]\n",
      "18/12/14 00:27:30 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "18/12/14 00:27:30 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "18/12/14 00:27:30 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "18/12/14 00:27:30 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "18/12/14 00:27:30 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "18/12/14 00:27:30 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "18/12/14 00:27:30 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "18/12/14 00:27:30 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "18/12/14 00:27:30 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "18/12/14 00:27:30 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "18/12/14 00:27:30 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "18/12/14 00:27:30 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "18/12/14 00:27:30 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/14 00:27:30 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/14 00:27:30 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/14 00:27:30 INFO streaming.PipeMapRed: Records R/W=443/1\n",
      "18/12/14 00:27:30 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/12/14 00:27:30 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/12/14 00:27:30 INFO mapred.LocalJobRunner: \n",
      "18/12/14 00:27:30 INFO mapred.MapTask: Starting flush of map output\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: Spilling map output\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: bufstart = 0; bufend = 82; bufvoid = 104857600\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: Finished spill 0\n",
      "18/12/14 00:27:30 INFO mapred.Task: Task:attempt_local1818403589_0001_m_000000_0 is done. And is in the process of committing\n",
      "18/12/14 00:27:30 INFO mapred.LocalJobRunner: Records R/W=443/1\n",
      "18/12/14 00:27:30 INFO mapred.Task: Task 'attempt_local1818403589_0001_m_000000_0' done.\n",
      "18/12/14 00:27:30 INFO mapred.Task: Final Counters for attempt_local1818403589_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=67640\n",
      "\t\tFILE: Number of bytes written=385931\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=443\n",
      "\t\tMap output records=2\n",
      "\t\tMap output bytes=82\n",
      "\t\tMap output materialized bytes=92\n",
      "\t\tInput split bytes=116\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=28\n",
      "\t\tTotal committed heap usage (bytes)=137433088\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=64794\n",
      "18/12/14 00:27:30 INFO mapred.LocalJobRunner: Finishing task: attempt_local1818403589_0001_m_000000_0\n",
      "18/12/14 00:27:30 INFO mapred.LocalJobRunner: Starting task: attempt_local1818403589_0001_m_000001_0\n",
      "18/12/14 00:27:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/14 00:27:30 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/14 00:27:30 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: Processing split: file:/home/comp6235/Notebooks/CW2/test_files/Youtube02-KatyPerry.csv:0+64654\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: numReduceTasks: 1\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: soft limit at 83886080\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "18/12/14 00:27:30 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "18/12/14 00:27:30 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/CW2/./mapper1.py, Mini]\n",
      "18/12/14 00:27:30 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/14 00:27:30 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/14 00:27:31 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/14 00:27:31 INFO streaming.PipeMapRed: Records R/W=355/1\n",
      "18/12/14 00:27:31 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/12/14 00:27:31 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/12/14 00:27:31 INFO mapred.LocalJobRunner: \n",
      "18/12/14 00:27:31 INFO mapred.MapTask: Starting flush of map output\n",
      "18/12/14 00:27:31 INFO mapred.MapTask: Spilling map output\n",
      "18/12/14 00:27:31 INFO mapred.MapTask: bufstart = 0; bufend = 82; bufvoid = 104857600\n",
      "18/12/14 00:27:31 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n",
      "18/12/14 00:27:31 INFO mapred.MapTask: Finished spill 0\n",
      "18/12/14 00:27:31 INFO mapred.Task: Task:attempt_local1818403589_0001_m_000001_0 is done. And is in the process of committing\n",
      "18/12/14 00:27:31 INFO mapred.LocalJobRunner: Records R/W=355/1\n",
      "18/12/14 00:27:31 INFO mapred.Task: Task 'attempt_local1818403589_0001_m_000001_0' done.\n",
      "18/12/14 00:27:31 INFO mapred.Task: Final Counters for attempt_local1818403589_0001_m_000001_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=132663\n",
      "\t\tFILE: Number of bytes written=386055\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=355\n",
      "\t\tMap output records=2\n",
      "\t\tMap output bytes=82\n",
      "\t\tMap output materialized bytes=92\n",
      "\t\tInput split bytes=120\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=26\n",
      "\t\tTotal committed heap usage (bytes)=184619008\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=64654\n",
      "18/12/14 00:27:31 INFO mapred.LocalJobRunner: Finishing task: attempt_local1818403589_0001_m_000001_0\n",
      "18/12/14 00:27:31 INFO mapred.LocalJobRunner: Starting task: attempt_local1818403589_0001_m_000002_0\n",
      "18/12/14 00:27:31 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/14 00:27:31 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/14 00:27:31 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/12/14 00:27:31 INFO mapred.MapTask: Processing split: file:/home/comp6235/Notebooks/CW2/test_files/Youtube01-Psy.csv:0+57785\n",
      "18/12/14 00:27:31 INFO mapred.MapTask: numReduceTasks: 1\n",
      "18/12/14 00:27:31 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "18/12/14 00:27:31 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "18/12/14 00:27:31 INFO mapred.MapTask: soft limit at 83886080\n",
      "18/12/14 00:27:31 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "18/12/14 00:27:31 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "18/12/14 00:27:31 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "18/12/14 00:27:31 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/CW2/./mapper1.py, Mini]\n",
      "18/12/14 00:27:31 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/14 00:27:31 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/14 00:27:31 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/14 00:27:31 INFO mapreduce.Job: Job job_local1818403589_0001 running in uber mode : false\n",
      "18/12/14 00:27:31 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/12/14 00:27:31 INFO streaming.PipeMapRed: Records R/W=355/1\n",
      "18/12/14 00:27:31 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/12/14 00:27:31 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/12/14 00:27:31 INFO mapred.LocalJobRunner: \n",
      "18/12/14 00:27:31 INFO mapred.MapTask: Starting flush of map output\n",
      "18/12/14 00:27:31 INFO mapred.MapTask: Spilling map output\n",
      "18/12/14 00:27:31 INFO mapred.MapTask: bufstart = 0; bufend = 68; bufvoid = 104857600\n",
      "18/12/14 00:27:31 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n",
      "18/12/14 00:27:31 INFO mapred.MapTask: Finished spill 0\n",
      "18/12/14 00:27:31 INFO mapred.Task: Task:attempt_local1818403589_0001_m_000002_0 is done. And is in the process of committing\n",
      "18/12/14 00:27:31 INFO mapred.LocalJobRunner: Records R/W=355/1\n",
      "18/12/14 00:27:31 INFO mapred.Task: Task 'attempt_local1818403589_0001_m_000002_0' done.\n",
      "18/12/14 00:27:31 INFO mapred.Task: Final Counters for attempt_local1818403589_0001_m_000002_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=190817\n",
      "\t\tFILE: Number of bytes written=386165\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=355\n",
      "\t\tMap output records=2\n",
      "\t\tMap output bytes=68\n",
      "\t\tMap output materialized bytes=78\n",
      "\t\tInput split bytes=114\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=24\n",
      "\t\tTotal committed heap usage (bytes)=169426944\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=57785\n",
      "18/12/14 00:27:31 INFO mapred.LocalJobRunner: Finishing task: attempt_local1818403589_0001_m_000002_0\n",
      "18/12/14 00:27:31 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "18/12/14 00:27:31 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "18/12/14 00:27:31 INFO mapred.LocalJobRunner: Starting task: attempt_local1818403589_0001_r_000000_0\n",
      "18/12/14 00:27:31 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/14 00:27:31 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/14 00:27:31 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/12/14 00:27:31 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@16726b3\n",
      "18/12/14 00:27:31 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "18/12/14 00:27:31 INFO reduce.EventFetcher: attempt_local1818403589_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "18/12/14 00:27:31 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1818403589_0001_m_000002_0 decomp: 74 len: 78 to MEMORY\n",
      "18/12/14 00:27:31 INFO reduce.InMemoryMapOutput: Read 74 bytes from map-output for attempt_local1818403589_0001_m_000002_0\n",
      "18/12/14 00:27:31 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 74, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->74\n",
      "18/12/14 00:27:31 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1818403589_0001_m_000000_0 decomp: 88 len: 92 to MEMORY\n",
      "18/12/14 00:27:31 INFO reduce.InMemoryMapOutput: Read 88 bytes from map-output for attempt_local1818403589_0001_m_000000_0\n",
      "18/12/14 00:27:31 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 88, inMemoryMapOutputs.size() -> 2, commitMemory -> 74, usedMemory ->162\n",
      "18/12/14 00:27:31 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1818403589_0001_m_000001_0 decomp: 88 len: 92 to MEMORY\n",
      "18/12/14 00:27:31 INFO reduce.InMemoryMapOutput: Read 88 bytes from map-output for attempt_local1818403589_0001_m_000001_0\n",
      "18/12/14 00:27:31 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 88, inMemoryMapOutputs.size() -> 3, commitMemory -> 162, usedMemory ->250\n",
      "18/12/14 00:27:31 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "18/12/14 00:27:31 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "18/12/14 00:27:31 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "18/12/14 00:27:31 INFO mapred.Merger: Merging 3 sorted segments\n",
      "18/12/14 00:27:31 INFO mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 229 bytes\n",
      "18/12/14 00:27:31 INFO reduce.MergeManagerImpl: Merged 3 segments, 250 bytes to disk to satisfy reduce memory limit\n",
      "18/12/14 00:27:31 INFO reduce.MergeManagerImpl: Merging 1 files, 250 bytes from disk\n",
      "18/12/14 00:27:31 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "18/12/14 00:27:31 INFO mapred.Merger: Merging 1 sorted segments\n",
      "18/12/14 00:27:31 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 239 bytes\n",
      "18/12/14 00:27:31 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "18/12/14 00:27:31 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/CW2/././reducer1.py]\n",
      "18/12/14 00:27:31 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "18/12/14 00:27:31 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "18/12/14 00:27:31 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/14 00:27:31 INFO streaming.PipeMapRed: Records R/W=6/1\n",
      "18/12/14 00:27:31 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/12/14 00:27:31 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/12/14 00:27:31 INFO mapred.Task: Task:attempt_local1818403589_0001_r_000000_0 is done. And is in the process of committing\n",
      "18/12/14 00:27:31 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "18/12/14 00:27:31 INFO mapred.Task: Task attempt_local1818403589_0001_r_000000_0 is allowed to commit now\n",
      "18/12/14 00:27:31 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1818403589_0001_r_000000_0' to file:/home/comp6235/Notebooks/CW2/output2/_temporary/0/task_local1818403589_0001_r_000000\n",
      "18/12/14 00:27:31 INFO mapred.LocalJobRunner: Records R/W=6/1 > reduce\n",
      "18/12/14 00:27:31 INFO mapred.Task: Task 'attempt_local1818403589_0001_r_000000_0' done.\n",
      "18/12/14 00:27:31 INFO mapred.Task: Final Counters for attempt_local1818403589_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=191425\n",
      "\t\tFILE: Number of bytes written=386665\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=262\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=6\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=169426944\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=250\n",
      "18/12/14 00:27:31 INFO mapred.LocalJobRunner: Finishing task: attempt_local1818403589_0001_r_000000_0\n",
      "18/12/14 00:27:31 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "18/12/14 00:27:32 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/12/14 00:27:32 INFO mapreduce.Job: Job job_local1818403589_0001 completed successfully\n",
      "18/12/14 00:27:32 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=582545\n",
      "\t\tFILE: Number of bytes written=1544816\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1153\n",
      "\t\tMap output records=6\n",
      "\t\tMap output bytes=232\n",
      "\t\tMap output materialized bytes=262\n",
      "\t\tInput split bytes=350\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=262\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=12\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=78\n",
      "\t\tTotal committed heap usage (bytes)=660905984\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=187233\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=250\n",
      "18/12/14 00:27:32 INFO streaming.StreamJob: Output directory: output2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Clear output\n",
    "rm -rf output2\n",
    "\n",
    "# Make sure hadoop is in standalone mode\n",
    "hadoop-standalone-mode.sh\n",
    "\n",
    "# Main pipeline command\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "-files mapper1.py,reducer1.py \\\n",
    "-input ./test_files/Youtube01-Psy.csv ./test_files/Youtube02-KatyPerry.csv ./test_files/Youtube03-LMFAO.csv \\\n",
    "-mapper 'mapper1.py Mini' -file ./mapper1.py  \\\n",
    "-reducer ./reducer1.py \\\n",
    "-output output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Expected key-value output format:\n",
    "#John Smith\t[\"Comment 1\", \"Comment 2\", \"Comment 3\", \"etc.\"]\n",
    "#Jane Doe\t[\"Comment 1\", \"Comment 2\", \"Comment 3\", \"etc.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
