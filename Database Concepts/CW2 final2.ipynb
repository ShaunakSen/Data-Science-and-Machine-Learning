{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2: Data Processing\n",
    "\n",
    "## Task 1\n",
    "This coursework will assess your understanding of using NoSQL to store and retrieve data.  You will perform operations on data from the Enron email dataset in a MongoDB database, and write a report detailing the suitability of different types of databases for data science applications.  You will be required to run code to answer the given questions in the Jupyter notebook provided, and write a report describing alternative approaches to using MongoDB.\n",
    "\n",
    "Download the JSON version of the Enron data (using the “Download as zip” to download the data file from http://edshare.soton.ac.uk/19548/, the file is about 380MB) and import into a collection called messages in a database called enron.  You do not need to set up any authentication.  In the Jupyter notebook provided, perform the following tasks, using the Python PyMongo library.\n",
    "\n",
    "Answers should be efficient in terms of speed.  Answers which are less efficient will not get full marks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset\n",
    "\n",
    "The JSON version of the dataset has been downloaded from [this link](http://edshare.soton.ac.uk/19548/)\n",
    "\n",
    "The dataset has been imported into the database **enron**\n",
    "\n",
    "The name of the collection is **messages**\n",
    "\n",
    "**100000** documents have been imported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE CODE TO IMPORT FULL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-15T19:34:02.061+0000\tFailed: open ./messages_short.json: no such file or directory\n",
      "2018-12-15T19:34:02.061+0000\timported 0 documents\n",
      "rm: cannot remove './messages_short.json': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# mongoimport is the Mongo command to import data.  \n",
    "# It specifies the database, collection and format, and import file\n",
    "# --drop means it's going to drop any collection with the same name which already exists\n",
    "mongoimport --db enron_short --collection messages --drop --file ./messages_short.json\n",
    "# Delete the JSON file we just downloaded\n",
    "rm ./messages_short.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'local']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MongoClient('mongodb://localhost:27017')\n",
    "\n",
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)\n",
    "Write a function which returns a MongoDB connection object to the \"messages\" collection. [4 points] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change DB Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "get_collection",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Database:', 'enron_short', 'not found')\n"
     ]
    }
   ],
   "source": [
    "db_name = 'enron_short'\n",
    "coll_name = 'messages'\n",
    "\n",
    "def get_collection():\n",
    "    \"\"\"\n",
    "    Connects to the server, and returns a collection object\n",
    "    of the `messages` collection in the `enron` database\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    client = MongoClient('mongodb://localhost:27017')\n",
    "    \n",
    "    # check if the database is present\n",
    "    if db_name in client.list_database_names():\n",
    "        db = client[db_name]\n",
    "        # check if collection is present\n",
    "        if coll_name in db.list_collection_names():\n",
    "            collection = db[coll_name]\n",
    "        else:\n",
    "            print(\"Collection:\", coll_name, \"not found\")\n",
    "            return False\n",
    "    else:\n",
    "        print (\"Database:\", db_name, \"not found\")\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    return collection\n",
    "            \n",
    "        \n",
    "messages_collection = get_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying that collection connection is able to read all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'count_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2e14a2d8ceea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmessages_collection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'count_documents'"
     ]
    }
   ],
   "source": [
    "messages_collection.count_documents({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)\n",
    "\n",
    "Write a function which returns the amount of emails in the messages collection in total. [4 points] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "get_amount_of_messages",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_amount_of_messages(collection):\n",
    "    \"\"\"\n",
    "    :param collection A PyMongo collection object\n",
    "    :return the amount of documents in the collection\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "    return collection.count_documents({})\n",
    "number_of_emails = get_amount_of_messages(messages_collection)    \n",
    "\n",
    "print (number_of_emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) \n",
    "\n",
    "Write a function which returns each person who was BCCed on an email.  Include each person only once, and display only their name according to the X-To header. [4 points] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "get_bcced_people",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_bcced_people(collection):\n",
    "    \"\"\"\n",
    "    :param collection A PyMongo collection object\n",
    "    :return the names of the people who have received an email by BCC\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # lists to store intermediate output\n",
    "    bcc_list = []\n",
    "    bcc_list1 = []\n",
    "    bcc_list2 = []\n",
    "    bcc_list3 = []\n",
    "    \n",
    "    \n",
    "    # final list of names\n",
    "    final_list = []\n",
    "\n",
    "    # find docs where bcc field exists and is not empty and append in the list: bcc_list\n",
    "\n",
    "    for doc in collection.find({ 'headers.X-bcc': {'$exists': True, '$ne': ''} }):\n",
    "        bcc_list.append(doc['headers']['X-bcc'])\n",
    "\n",
    "\n",
    "    # set of operations to clean the data\n",
    "\n",
    "    for bcc_value in bcc_list:\n",
    "        bcc_list1.append(bcc_value.split('>,'))\n",
    "\n",
    "\n",
    "\n",
    "    for bcc_value in bcc_list1:\n",
    "        for value in bcc_value:\n",
    "            bcc_list2.append(value.split('</O')[0])\n",
    "\n",
    "\n",
    "    for bcc_value in bcc_list2:\n",
    "        bcc_list3.append(bcc_value.split(' <')[0])\n",
    "\n",
    "    # now we have set of names and emails without the part between '<>'\n",
    "\n",
    "    # we want only the names\n",
    "\n",
    "    for bcc_value in bcc_list3:\n",
    "        # ignore the email ids\n",
    "        if '@' not in bcc_value:\n",
    "            # strip the names of trailing whitespaces and check if the names are already in the final list \n",
    "            if bcc_value.strip() not in final_list:\n",
    "                final_list.append(bcc_value.strip())\n",
    "\n",
    "    return final_list\n",
    "get_bcced_people(messages_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following questions I have used the **Aggregation Pipeline** framework of MongoDB to process and aggregate the data into logical steps for easier debugging and improved performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)\n",
    "\n",
    "Write a function with parameter subject, which gets all emails in a thread with that parameter, and orders them by date (ascending). “An email thread is an email message that includes a running list of all the succeeding replies starting with the original email.”, check for detail descriptions at https://www.techopedia.com/definition/1503/email-thread [4 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "get_emails_in_thread",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_emails_in_thread(collection, subject):\n",
    "    \"\"\"\n",
    "    :param collection A PyMongo collection object\n",
    "    :return All emails in the thread with that subject\n",
    "    \"\"\" \n",
    "    \n",
    "    # The output should include all emails in the thread including the original email\n",
    "    # YOUR CODE HERE    \n",
    "    \n",
    "    output_emails = []\n",
    "    \n",
    "    # Get the main subject as parameter\n",
    "\n",
    "\n",
    "    # other mails in the thread will have the subject : 'Re: [subject]'\n",
    "    subject = subject.strip()\n",
    "\n",
    "    reply_subject = \"Re: \" + subject\n",
    "\n",
    "    # limit stage to limit results (may be added to the pipeline for debugging)\n",
    "\n",
    "    limit_stage = {\n",
    "        '$limit': 100\n",
    "    }\n",
    "\n",
    "    # This stage is to match the emails according to the subjects\n",
    "\n",
    "    match_stage = {\n",
    "        '$match': { 'headers.Subject': { '$in': [subject, reply_subject] } }\n",
    "    }\n",
    "\n",
    "    # This stage does the following operations:\n",
    "    # 1. $substr: Extracts the first 25 chars from the Date Field\n",
    "    # 2. $rtrim: removes trailing whitespace that appears if the day is a single digit number\n",
    "    # 3. $dateFromString: converts the string to a Date object, which will be used in the sorting stage\n",
    "    # I have also used this stage to display certain fields so that the output looks clean\n",
    "\n",
    "    project_stage = {\n",
    "         '$project': { 'DateOfMessage': \n",
    "                          {'$dateFromString': { 'dateString' : \n",
    "                                               { '$rtrim': {'input': {'$substr': ['$headers.Date', 0, 25] }}}\n",
    "\n",
    "                                              }\n",
    "                          },\n",
    "\n",
    "                        'filename': 1,\n",
    "                        'body': 1,\n",
    "                        'Date': '$headers.Date',\n",
    "                        'Subject': '$headers.Subject'\n",
    "                     } \n",
    "    }\n",
    "\n",
    "    # This stage sorts the docs in ascending order of Date\n",
    "\n",
    "    sort_stage = {\n",
    "        '$sort': {'DateOfMessage': 1}\n",
    "    }\n",
    "\n",
    "    #\n",
    "\n",
    "    project_stage2 = {\n",
    "        '$project': {\n",
    "            'DateOfMessage': 0,\n",
    "            '_id': 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    pipeline = [match_stage, project_stage, sort_stage, project_stage2]\n",
    "\n",
    "    # return the cursor\n",
    "    return collection.aggregate(pipeline)\n",
    "\n",
    "for doc in get_emails_in_thread(messages_collection, \"Plays and other information\"):\n",
    "    pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)\n",
    "\n",
    "Write a function which returns the percentage of emails sent on a weekend (i.e., Saturday and Sunday) as a `float` between 0 and 1. [6 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "get_percentage_sent_on_weekend",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_percentage_sent_on_weekend(collection):\n",
    "    \"\"\"\n",
    "    :param collection A PyMongo collection object\n",
    "    :return A float between 0 and 1\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # compute total number of emails in our dataset\n",
    "    \n",
    "    total_documents = float(collection.count_documents({}))\n",
    "    \n",
    "    # limit stage to limit results (may be added to the pipeline for debugging)\n",
    "\n",
    "    limit_stage = {\n",
    "        '$limit': 100\n",
    "    }\n",
    "\n",
    "    # This stage does the following operations:\n",
    "    # 1. $substr: Extracts the first 25 chars from the Date Field\n",
    "    # 2. $rtrim: removes trailing whitespace that appears if the day is a single digit number\n",
    "    # 3. $dateFromString: converts the string to a Date object\n",
    "    project_stage = {\n",
    "         '$project': { 'DateOfMessage': \n",
    "                          {'$dateFromString': { 'dateString' : \n",
    "                                               { '$rtrim': {'input': {'$substr': ['$headers.Date', 0, 25] }}}\n",
    "\n",
    "                                              }\n",
    "                          }\n",
    "                     } \n",
    "    }\n",
    "    \n",
    "    \n",
    "    # This stage computes the day of the week for a date as a number between 1 (Sunday) and 7 (Saturday)\n",
    "    project_stage2 = {\n",
    "         '$project': {\n",
    "                        'DayOfWeek': {\n",
    "                            '$dayOfWeek': '$DateOfMessage'\n",
    "                        }\n",
    "                     } \n",
    "    }\n",
    "    \n",
    "\n",
    "    # This stage is used to filter docs sent on Sunday(1) or Saturday(7)\n",
    "    match_stage1 = {\n",
    "            '$match': { 'DayOfWeek': { '$in': [1, 7] } }\n",
    "    }\n",
    "\n",
    "    # Group stage to count the number of documents\n",
    "    group_stage1 = {\n",
    "        '$group': {\n",
    "            '_id': None, 'count': {'$sum': 1}\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    # final project stage to compute percentage of emails sent on weekends    \n",
    "    project_stage4 = {\n",
    "        '$project': {\n",
    "            'percentage_weekend': { '$divide': ['$count', total_documents] }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pipeline = [project_stage, project_stage2, match_stage1, group_stage1, project_stage4]\n",
    "\n",
    "    for doc in collection.aggregate(pipeline):\n",
    "        return float(doc['percentage_weekend'])\n",
    "\n",
    "get_percentage_sent_on_weekend(messages_collection)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6)\n",
    "\n",
    "Write a function with parameter limit. The function should return for each email account: the number of emails sent, the number of emails received, and the total number of emails (sent and received). Use the following format: [{\"contact\": \"michael.simmons@enron.com\", \"from\": 42, \"to\": 92, \"total\": 134}] and the information contained in the To, From, and Cc headers. Sort the output in descending order by the total number of emails. Use the parameter limit to specify the number of results to be returned. If limit is null, the function should return all results. If limit is higher than null, the function should return the number of results specified as limit. limit cannot take negative values. [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "get_emails_between_contacts",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_emails_between_contacts(collection, limit):\n",
    "    \"\"\"\n",
    "    Shows the communications between contacts\n",
    "    Sort by the descending order of total emails using the To, From, and Cc headers.\n",
    "    :param `collection` A PyMongo collection object    \n",
    "    :param `limit` An integer specifying the amount to display, or\n",
    "    if null will display all outputs\n",
    "    :return A list of objects of the form:\n",
    "    [{\n",
    "        'contact': <<Another email address>>\n",
    "        'from': \n",
    "        'to': \n",
    "        'total': \n",
    "    },{.....}]\n",
    "    \"\"\"    \n",
    "    \n",
    "    from_data = []\n",
    "    to_data = []\n",
    "\n",
    "    from_emails = []\n",
    "    to_emails = []\n",
    "\n",
    "    common_emails = []\n",
    "\n",
    "    # limit stage to limit results (may be added to the pipeline for debugging)\n",
    "\n",
    "    limit_stage = {\n",
    "        '$limit': 100\n",
    "    }\n",
    "\n",
    "    final_list = []\n",
    "\n",
    "\n",
    "\n",
    "    # In the first stage we project the fields \"From\" and \"To\" for each email \n",
    "    # From: sender of the email (headers.From)\n",
    "    # To: receivers (To + Cc) of the email (headers.To + headers.Cc)\n",
    "\n",
    "    project_stage1 = {\n",
    "         '$project': {\n",
    "                         '_id': 0,\n",
    "                         'To': ['$headers.To', '$headers.Cc'],\n",
    "                         'From': '$headers.From',\n",
    "                     } \n",
    "    }\n",
    "\n",
    "    # Unwind stage: Deconstructs the field: 'To' to output a document for each element\n",
    "\n",
    "    unwind_stage1 = {\n",
    "        '$unwind': {'path': '$To'}\n",
    "    }\n",
    "\n",
    "    # After the unwind stage, there are docs of the form: {'To': None, 'From': 'michael.simmons@enron.com'}\n",
    "    # We filter out these docs with 'To': None\n",
    "\n",
    "    match_stage1 = {\n",
    "        '$match': {\n",
    "            'To': {'$ne': None}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # There are docs which have multiple ids in 'To field', separated by ', '. So we split them \n",
    "\n",
    "    project_stage2 = {\n",
    "        '$project': {\n",
    "                         'From': 1,\n",
    "                         'To': {'$split': ['$To', ', ']}\n",
    "\n",
    "                     } \n",
    "    }\n",
    "\n",
    "    # Another Unwind stage: Deconstructs the field: 'To' to output a document for each element\n",
    "\n",
    "    unwind_stage2 = {\n",
    "        '$unwind': {'path': '$To'}\n",
    "    }\n",
    "\n",
    "\n",
    "    # Now we have single ids in 'From' and 'To' fields \n",
    "\n",
    "    # But some ids in 'To' field are of the form: To': '\\r\\n\\tbryan.hull@enron.com'\n",
    "    # So we use $trim to get rid of these unwanted characters\n",
    "\n",
    "\n",
    "    project_stage3 = {\n",
    "        '$project': {\n",
    "            'To': {'$trim': {'input': '$To'}},\n",
    "            'From': 1\n",
    "\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Now we have documents of the form {'From': email_id1, 'To': email_id2} \n",
    "    # where each document represents an email between the 2 email ids\n",
    "\n",
    "    # Stage to group the accounts according to 'To' field and compute the count (the number of emails received)\n",
    "\n",
    "\n",
    "    group_stage1 = {\n",
    "        '$group': { \n",
    "            '_id': '$To',\n",
    "            'count_to': {'$sum': 1}          \n",
    "                  }\n",
    "    }\n",
    "\n",
    "    # Stage to group the accounts according to 'From' field and compute the count (the number of emails sent)\n",
    "\n",
    "\n",
    "    group_stage2 = {\n",
    "        '$group': { \n",
    "            '_id': '$From',\n",
    "            'count_from': {'$sum': 1}          \n",
    "                  }\n",
    "    }\n",
    "\n",
    "\n",
    "    # We create two pripelines with the above mentioned stages\n",
    "    # pipeline1: Compute docs for each email id and number of emails received by the id (count_to) \n",
    "    # pipeline2: Compute docs for each email id and number of emails sent by the id (count_from) \n",
    "\n",
    "\n",
    "    pipeline1 = [ project_stage1, unwind_stage1, match_stage1, project_stage2, \n",
    "                     unwind_stage2, project_stage3, group_stage1]\n",
    "\n",
    "    pipeline2 = [ project_stage1, unwind_stage1, match_stage1, project_stage2, \n",
    "                     unwind_stage2, project_stage3, group_stage2]\n",
    "\n",
    "\n",
    "    # Append the docs in two separate lists \n",
    "    for doc in collection.aggregate(pipeline1):\n",
    "        to_data.append(doc)\n",
    "\n",
    "    for doc in collection.aggregate(pipeline2):\n",
    "        from_data.append(doc)\n",
    "\n",
    "\n",
    "    # Create two lists that contains the email ids\n",
    "    for user_data in to_data:\n",
    "        email = user_data['_id']\n",
    "        to_emails.append(email)\n",
    "\n",
    "\n",
    "    for user_data in from_data:\n",
    "        email = user_data['_id']\n",
    "        from_emails.append(email)\n",
    "\n",
    "    # for each account in the 'to' data, add the entry to a final list \n",
    "\n",
    "    for user_data in to_data:\n",
    "        contact = user_data['_id']\n",
    "        to_value = user_data['count_to']\n",
    "        # total is set as to_value initially\n",
    "        dict_entry = {'contact': contact, 'to': to_value, 'from': 0, 'total': to_value}\n",
    "        final_list.append(dict_entry)\n",
    "\n",
    "    # now we check each account in the 'from' data\n",
    "    # if the account is already there in the 'to' email ids, then the entry must have been added in the previous step\n",
    "    # so simply modify values (set 'from' value and add it to the 'total' value)\n",
    "    # else create a new entry and append to the final list\n",
    "\n",
    "    for user_data in from_data:\n",
    "        contact = user_data['_id']\n",
    "        from_value = user_data['count_from']\n",
    "        if contact in to_emails:\n",
    "            # already entry created,just modify values\n",
    "            for user_data in final_list:\n",
    "                if user_data['contact'] == contact:\n",
    "                    # set 'From' field\n",
    "                    user_data['from'] = from_value\n",
    "                    # update 'Total' field\n",
    "                    user_data['total'] += from_value\n",
    "\n",
    "        else:\n",
    "            # new user data\n",
    "            dict_entry = {'contact': contact, 'to': 0, 'from': from_value, 'total': from_value}\n",
    "            final_list.append(dict_entry)\n",
    "\n",
    "    sorted_list = sorted(final_list, key=lambda k: k['total'], reverse=True) \n",
    "    \n",
    "    if limit is None:\n",
    "    \n",
    "        return sorted_list\n",
    "    else:\n",
    "        limit = int(limit)\n",
    "        return sorted_list[:limit]\n",
    "\n",
    "get_emails_between_contacts(messages_collection, 5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7)\n",
    "Write a function to find out the number of senders who were also direct receivers. Direct receiver means the email is sent to the person directly, not via cc or bcc. [4 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_from_to_people(collection):\n",
    "    \"\"\"\n",
    "    :param collection A PyMongo collection object\n",
    "    :return the NUMBER of the people who have sent emails and received emails as direct receivers.\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    #------------- FIND DIRECT RECEIVERS ---------------------------------\n",
    "\n",
    "    direct_receivers = []\n",
    "\n",
    "    direct_receivers1 = []\n",
    "\n",
    "    # find docs which have the headers.To field and the field is not empty and append the data to a list\n",
    "\n",
    "    for doc in collection.find({ 'headers.To': {'$exists': True, '$ne': ''} }):\n",
    "        direct_receivers.append(doc['headers']['To'])\n",
    "\n",
    "    # clean up the data and store the unique valyes in a new list\n",
    "\n",
    "    for receivers in direct_receivers:\n",
    "        for receiver in receivers.split(', '):\n",
    "            if receiver.strip(' \\t\\n\\r') not in direct_receivers1:\n",
    "                direct_receivers1.append(receiver.strip(' \\t\\n\\r'))\n",
    "                \n",
    "    # direct_receivers1 contains the unique list of direct reciver email ids\n",
    "    \n",
    "    #------------- FIND UNIQUE SENDERS ---------------------------------\n",
    "\n",
    "    senders = []\n",
    "\n",
    "    unique_senders = []\n",
    "\n",
    "    # find docs which have the headers.From field and the field is not empty and append the data to a list\n",
    "\n",
    "\n",
    "    for doc in collection.find({ 'headers.From': {'$exists': True, '$ne': ''} }):\n",
    "        senders.append(doc['headers']['From'].strip())\n",
    "\n",
    "    # store uniqie sender email ids in a list\n",
    "    for sender in senders:\n",
    "        if sender not in unique_senders:\n",
    "            unique_senders.append(sender)\n",
    "\n",
    "\n",
    "\n",
    "    #------------- FIND THE NUMBER OF COMMON ELEMENTS ---------------------------------\n",
    "\n",
    "    return len(set(direct_receivers1).intersection(senders1))\n",
    "\n",
    "get_from_to_people(messages_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8)\n",
    "Write a function with parameters start_date and end_date, which returns the number of email messages that have been sent between those specified dates, including start_date and end_date [4 points] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function can accept dates in the following formats (with or without timezone specified)\n",
    "\n",
    "- Tue, 14 Nov 2000 08:22:00 -0800 (PST)\n",
    "- Tue, 14 Nov 2000 08:22:00\n",
    "\n",
    "Whichever form is used, the function converts it into proper datetime format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_emails_between_dates(collection, start_date, end_date):\n",
    "    \"\"\"\n",
    "    :param collection A PyMongo collection object\n",
    "    :return All emails between the specified start_date and end_date\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE \n",
    "    \n",
    "    start_date_as_date = ''\n",
    "    end_date_as_date = ''\n",
    "    \n",
    "    # start date and end date to be in form: Tue, 14 Nov 2000 08:22:00 -0800 (PST)\n",
    "    # or in form: Tue, 14 Nov 2000 08:22:00\n",
    "    \n",
    "    start_date = start_date.strip()\n",
    "    end_date = end_date.strip()\n",
    "    \n",
    "    # check for format used and remove timezone if present\n",
    "    \n",
    "    \n",
    "    if start_date[25:].strip() == '-0800 (PST)':\n",
    "        start_date = start_date[:25].strip()\n",
    "        \n",
    "    if end_date[25:].strip() == '-0800 (PST)':\n",
    "        end_date = end_date[:25].strip()\n",
    "    \n",
    "    if len(start_date) <= 25 and len(end_date) <= 25:\n",
    "        # parse as datetime\n",
    "        start_date_as_date = datetime.strptime(start_date, '%a, %d %b %Y %H:%M:%S')\n",
    "        end_date_as_date = datetime.strptime(end_date, '%a, %d %b %Y %H:%M:%S')\n",
    "    \n",
    "    # limit stage to limit results (may be added to the pipeline for debugging)\n",
    "    \n",
    "    limit_stage = {\n",
    "    '$limit': 100\n",
    "    }\n",
    "\n",
    "\n",
    "    # This stage does the following operations:\n",
    "    # 1. $substr: Extracts the first 25 chars from the Date Field\n",
    "    # 2. $rtrim: removes trailing whitespace that appears if the day is a single digit number\n",
    "    # 3. $dateFromString: converts the string to a Date object\n",
    "    \n",
    "    project_stage = {\n",
    "         '$project': { 'DateOfMessage': \n",
    "                          {'$dateFromString': { 'dateString' : \n",
    "                                               { '$rtrim': {'input': {'$substr': ['$headers.Date', 0, 25] }}}\n",
    "\n",
    "                                              }\n",
    "                          }\n",
    "                     } \n",
    "    }\n",
    "    \n",
    "    # match stage to filter documents with Date >= start_date and Date <= end_date\n",
    "    \n",
    "    match_stage = {\n",
    "        '$match': {\n",
    "            '$and': [ {'DateOfMessage' : { '$gte': start_date_as_date } },  \n",
    "                      {'DateOfMessage' : { '$lte': end_date_as_date } } ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # group stage to count number of emails\n",
    "    \n",
    "    group_stage1 = {\n",
    "        '$group': {\n",
    "            '_id': None, 'count': {'$sum': 1}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    pipeline = [project_stage, match_stage, group_stage1]\n",
    "    \n",
    "    for doc in collection.aggregate(pipeline):\n",
    "        return int(doc['count'])\n",
    "    \n",
    "    \n",
    "get_emails_between_dates(messages_collection, \"Sat, 11 Nov 2000 16:37:00\", \"Tue, 14 Nov 2000 08:22:00 -0800 (PST)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "This task will assess your ability to use the Hadoop Streaming API and MapReduce to process data. For each of the questions below, you are expected to write two python scripts, one for the Map phase and one for the Reduce phase. You are also expected to provide the correct parameters to the `hadoop` command to run the MapReduce process. Write down your answers in the specified cells below.\n",
    "\n",
    "To get started, you need to download and unzip the YouTube dataset (available at http://edshare.soton.ac.uk/19547/) onto the machine where you have Hadoop installed (this should be the virtual machine provided).\n",
    "\n",
    "To help you, `%%writefile` has been added to the top of the cells, automatically writing them to \"mapper.py\" and \"reducer.py\" respectively when the cells are run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) \n",
    "Using Youtube01-Psy.csv, find the hourly interval in which most spam was sent. The output should be in the form of a single key-value pair, where the value is a datetime at the start of the hour with the highest number of spam comments. [9 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper1.py\n",
    "#!/usr/bin/env python\n",
    "#Answer for mapper.py\n",
    "\n",
    "\n",
    "# import the libraries\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# read in through standard input\n",
    "lines = sys.stdin.readlines()\n",
    "\n",
    "# csvreader is a reader object which will iterate over lines\n",
    "csvreader = csv.reader(lines)\n",
    "\n",
    "# Lists to hold the dates of the comments and the class (spam or not)\n",
    "dates = []\n",
    "\n",
    "spam_class = []\n",
    "\n",
    "# counter variable is used to skip first row which contains the headers\n",
    "counter = 0\n",
    "for row in csvreader:\n",
    "    # skip first row\n",
    "    if counter > 0:\n",
    "        dates.append(row[2])\n",
    "        spam_class.append(row[4])\n",
    "    counter += 1\n",
    "    \n",
    "# check for number of dates and spam classes\n",
    "if (len(dates) != len(spam_class)):\n",
    "    print ('Unequal number of entries in Date and Class columns... Aborting...')\n",
    "    sys.exit()\n",
    "\n",
    "# The mapper generates key\tvalue pairs for the reducer step to aggregate\n",
    "# We generate key\tvalue pairs of the form: day|month|year|hour\t1\n",
    "# Thus there will be a key\tvalue pair for each spam comment, \n",
    "# with the key specifying the day,month, year and hour of the comment\n",
    "\n",
    "for x in range(len(dates)):\n",
    "    if spam_class[x] == '1':\n",
    "        # remove leading or trailing whitespaces\n",
    "        date = dates[x].strip()\n",
    "        # convert to datetime format\n",
    "        date_as_date = datetime.strptime(date, '%Y-%m-%dT%H:%M:%S')\n",
    "        # find day, month, year and hour from the date\n",
    "        day = date_as_date.date().day\n",
    "        month = date_as_date.date().month\n",
    "        year = date_as_date.date().year\n",
    "        hour = date_as_date.hour\n",
    "        \n",
    "        # output the key\tvalue pair\n",
    "        print (str(day) + '|' + str(month) + '|' + str(year) + '|' + str(hour) + '\\t' + '1')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "#!/usr/bin/env python\n",
    "#Answer for reducer.py\n",
    "\n",
    "\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# we get the input pairs in key\tvalue format from the mapper step\n",
    "input_pairs = sys.stdin.readlines()\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "date_count_dict = dict()\n",
    "\n",
    "final_dict = {\n",
    "    'hour_with_most_spam': None\n",
    "}\n",
    "\n",
    "# iterate over each input pair and append the keys(dates) into a list\n",
    "for input_pair in input_pairs:\n",
    "    # split the input by '\\t' to separate out the key and the value\n",
    "    # the second param to split specifies that the maximum number of splits is 1\n",
    "    input_list = input_pair.split('\\t', 1)\n",
    "    # check if the splitted list contains 2 elements i.e a key and a value. If not, we skip for that input\n",
    "    if (len(input_list) != 2):\n",
    "        continue\n",
    "    \n",
    "    dates_list.append(input_list[0])\n",
    "        \n",
    "# next we build a dictionary which contains the key as the date and the value as the number of spam comments for that date\n",
    "# the key is in the format: day|month|year|hour \n",
    "\n",
    "for date in dates_list:\n",
    "    # if that date is already present, increase the count\n",
    "    if date in date_count_dict.keys():\n",
    "        date_count_dict[date] += 1\n",
    "    # if it is a new entry, set the count to 1\n",
    "    else:\n",
    "        date_count_dict[date] = 1\n",
    "        \n",
    "# sort the dictionary based on its values (count of spam comments) in DECREASING ORDER\n",
    "date_count_dict_sorted = sorted(date_count_dict.items(), key=lambda date_count_value: date_count_value[1], \n",
    "                                reverse=True)\n",
    "\n",
    "final_dict['hour_with_most_spam'] = date_count_dict_sorted[0][0]\n",
    "\n",
    "# some processing to get o/p in form: 2013-11-10T10:00:00\n",
    "# we split day|month|year|hour into 4 parts to separate out day, month, year and hour\n",
    "\n",
    "output_date = final_dict['hour_with_most_spam'].split('|',4)\n",
    "day = output_date[0]\n",
    "month = output_date[1]\n",
    "year = output_date[2]\n",
    "hour = output_date[3]\n",
    "final_output = str(year) + '-' + str(month) + '-' + str(day) + 'T' + str(hour) + ':00:00'\n",
    "\n",
    "# we set the value to the desired o/p format\n",
    "\n",
    "final_dict['hour_with_most_spam'] = final_output\n",
    "\n",
    "\n",
    "# print out the output in format: hour_with_most_spam\t\"2013-11-10T10:00:00\"\n",
    "for key, value in final_dict.items():\n",
    "    print (key + \"\\t\" + str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run the hadoop commands for the mapreduce\n",
    "\n",
    "The directory structure is:\n",
    "```\n",
    "-Notebooks\n",
    "    -\\output1\n",
    "    -mapper1.py\n",
    "    -reducer1.py\n",
    "    -Youtube01-Psy.csv\n",
    "    -Youtube02-KatyPerry.csv\n",
    "    -Youtube03-LMFAO.csv\n",
    "    -Youtube04-Eminem.csv\n",
    "    -Youtube05-Shakira.csv\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenJDK Server VM warning: You have loaded library /opt/hadoop-2.8.5/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.\n",
      "It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'.\n",
      "18/12/15 20:24:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "18/12/15 20:24:35 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "18/12/15 20:24:35 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "18/12/15 20:24:35 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "18/12/15 20:24:35 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "18/12/15 20:24:35 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "18/12/15 20:24:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2121305499_0001\n",
      "18/12/15 20:24:36 INFO mapred.LocalDistributedCacheManager: Localized file:/home/comp6235/Notebooks/CW2/mapper1.py as file:/tmp/hadoop-comp6235/mapred/local/1544905476130/mapper1.py\n",
      "18/12/15 20:24:36 INFO mapred.LocalDistributedCacheManager: Localized file:/home/comp6235/Notebooks/CW2/reducer1.py as file:/tmp/hadoop-comp6235/mapred/local/1544905476131/reducer1.py\n",
      "18/12/15 20:24:36 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "18/12/15 20:24:36 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "18/12/15 20:24:36 INFO mapreduce.Job: Running job: job_local2121305499_0001\n",
      "18/12/15 20:24:36 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "18/12/15 20:24:36 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/15 20:24:36 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/15 20:24:36 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "18/12/15 20:24:36 INFO mapred.LocalJobRunner: Starting task: attempt_local2121305499_0001_m_000000_0\n",
      "18/12/15 20:24:36 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/15 20:24:36 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/15 20:24:36 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/12/15 20:24:36 INFO mapred.MapTask: Processing split: file:/home/comp6235/Notebooks/CW2/Youtube01-Psy.csv:0+57438\n",
      "18/12/15 20:24:37 INFO mapred.MapTask: numReduceTasks: 1\n",
      "18/12/15 20:24:37 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "18/12/15 20:24:37 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "18/12/15 20:24:37 INFO mapred.MapTask: soft limit at 83886080\n",
      "18/12/15 20:24:37 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "18/12/15 20:24:37 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "18/12/15 20:24:37 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "18/12/15 20:24:37 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/CW2/././mapper1.py]\n",
      "18/12/15 20:24:37 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "18/12/15 20:24:37 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "18/12/15 20:24:37 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "18/12/15 20:24:37 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "18/12/15 20:24:37 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "18/12/15 20:24:37 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "18/12/15 20:24:37 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "18/12/15 20:24:37 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "18/12/15 20:24:37 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "18/12/15 20:24:37 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "18/12/15 20:24:37 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "18/12/15 20:24:37 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "18/12/15 20:24:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/15 20:24:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/15 20:24:37 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/15 20:24:37 INFO streaming.PipeMapRed: Records R/W=351/1\n",
      "18/12/15 20:24:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/12/15 20:24:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/12/15 20:24:37 INFO mapred.LocalJobRunner: \n",
      "18/12/15 20:24:37 INFO mapred.MapTask: Starting flush of map output\n",
      "18/12/15 20:24:37 INFO mapred.MapTask: Spilling map output\n",
      "18/12/15 20:24:37 INFO mapred.MapTask: bufstart = 0; bufend = 2604; bufvoid = 104857600\n",
      "18/12/15 20:24:37 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213700(104854800); length = 697/6553600\n",
      "18/12/15 20:24:37 INFO mapred.MapTask: Finished spill 0\n",
      "18/12/15 20:24:37 INFO mapred.Task: Task:attempt_local2121305499_0001_m_000000_0 is done. And is in the process of committing\n",
      "18/12/15 20:24:37 INFO mapred.LocalJobRunner: Records R/W=351/1\n",
      "18/12/15 20:24:37 INFO mapred.Task: Task 'attempt_local2121305499_0001_m_000000_0' done.\n",
      "18/12/15 20:24:37 INFO mapred.Task: Final Counters for attempt_local2121305499_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=195271\n",
      "\t\tFILE: Number of bytes written=524316\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=351\n",
      "\t\tMap output records=175\n",
      "\t\tMap output bytes=2604\n",
      "\t\tMap output materialized bytes=2960\n",
      "\t\tInput split bytes=103\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=175\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=29\n",
      "\t\tTotal committed heap usage (bytes)=137433088\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=57438\n",
      "18/12/15 20:24:37 INFO mapred.LocalJobRunner: Finishing task: attempt_local2121305499_0001_m_000000_0\n",
      "18/12/15 20:24:37 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "18/12/15 20:24:37 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "18/12/15 20:24:37 INFO mapred.LocalJobRunner: Starting task: attempt_local2121305499_0001_r_000000_0\n",
      "18/12/15 20:24:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/15 20:24:37 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/15 20:24:37 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/12/15 20:24:37 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1ae6b68\n",
      "18/12/15 20:24:37 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "18/12/15 20:24:37 INFO reduce.EventFetcher: attempt_local2121305499_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "18/12/15 20:24:37 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2121305499_0001_m_000000_0 decomp: 2956 len: 2960 to MEMORY\n",
      "18/12/15 20:24:37 INFO reduce.InMemoryMapOutput: Read 2956 bytes from map-output for attempt_local2121305499_0001_m_000000_0\n",
      "18/12/15 20:24:37 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2956, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2956\n",
      "18/12/15 20:24:37 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "18/12/15 20:24:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "18/12/15 20:24:37 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "18/12/15 20:24:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "18/12/15 20:24:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2940 bytes\n",
      "18/12/15 20:24:37 INFO reduce.MergeManagerImpl: Merged 1 segments, 2956 bytes to disk to satisfy reduce memory limit\n",
      "18/12/15 20:24:37 INFO reduce.MergeManagerImpl: Merging 1 files, 2960 bytes from disk\n",
      "18/12/15 20:24:37 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "18/12/15 20:24:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "18/12/15 20:24:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2940 bytes\n",
      "18/12/15 20:24:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "18/12/15 20:24:37 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/CW2/././reducer1.py]\n",
      "18/12/15 20:24:37 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "18/12/15 20:24:37 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "18/12/15 20:24:37 INFO mapreduce.Job: Job job_local2121305499_0001 running in uber mode : false\n",
      "18/12/15 20:24:37 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/12/15 20:24:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/15 20:24:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/15 20:24:37 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/15 20:24:37 INFO streaming.PipeMapRed: Records R/W=175/1\n",
      "18/12/15 20:24:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/12/15 20:24:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/12/15 20:24:37 INFO mapred.Task: Task:attempt_local2121305499_0001_r_000000_0 is done. And is in the process of committing\n",
      "18/12/15 20:24:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "18/12/15 20:24:37 INFO mapred.Task: Task attempt_local2121305499_0001_r_000000_0 is allowed to commit now\n",
      "18/12/15 20:24:37 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2121305499_0001_r_000000_0' to file:/home/comp6235/Notebooks/CW2/output1/_temporary/0/task_local2121305499_0001_r_000000\n",
      "18/12/15 20:24:37 INFO mapred.LocalJobRunner: Records R/W=175/1 > reduce\n",
      "18/12/15 20:24:37 INFO mapred.Task: Task 'attempt_local2121305499_0001_r_000000_0' done.\n",
      "18/12/15 20:24:37 INFO mapred.Task: Final Counters for attempt_local2121305499_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=201223\n",
      "\t\tFILE: Number of bytes written=527327\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=127\n",
      "\t\tReduce shuffle bytes=2960\n",
      "\t\tReduce input records=175\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=175\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=4\n",
      "\t\tTotal committed heap usage (bytes)=137433088\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=51\n",
      "18/12/15 20:24:37 INFO mapred.LocalJobRunner: Finishing task: attempt_local2121305499_0001_r_000000_0\n",
      "18/12/15 20:24:37 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "18/12/15 20:24:38 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/12/15 20:24:38 INFO mapreduce.Job: Job job_local2121305499_0001 completed successfully\n",
      "18/12/15 20:24:38 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=396494\n",
      "\t\tFILE: Number of bytes written=1051643\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=351\n",
      "\t\tMap output records=175\n",
      "\t\tMap output bytes=2604\n",
      "\t\tMap output materialized bytes=2960\n",
      "\t\tInput split bytes=103\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=127\n",
      "\t\tReduce shuffle bytes=2960\n",
      "\t\tReduce input records=175\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=350\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=33\n",
      "\t\tTotal committed heap usage (bytes)=274866176\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=57438\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=51\n",
      "18/12/15 20:24:38 INFO streaming.StreamJob: Output directory: output1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Clear output directory\n",
    "rm -rf output1\n",
    "\n",
    "# Main pipeline command\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "-files mapper1.py,reducer1.py \\\n",
    "-input ./Youtube01-Psy.csv \\\n",
    "-mapper ./mapper1.py \\\n",
    "-reducer ./reducer1.py \\\n",
    "-output output1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the above step runs we get the output in the directory: **output1** and in the file: **part-00000**\n",
    "\n",
    "The output is:\n",
    "\n",
    "```\n",
    "hour_with_most_spam\t2014-11-8T10:00:00\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expected key-value output format:\n",
    "#hour_with_most_spam\t\"2013-11-10T10:00:00\"\n",
    "\n",
    "#Additional key-value pairs are acceptable, as long as the hour_with_most_spam pair is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) \n",
    "Find all comments associated with a username (the AUTHOR field). Return a JSON array of all comments associated with that username. (This should use the data from all 5 data files: Psy, KatyPerry, LMFAO, Eminem, Shakira) [11 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/env python\n",
    "#Answer for mapper.py\n",
    "\n",
    "# importing the libraries\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "\n",
    "def mapper_function(required_username):\n",
    "\n",
    "    # function that accepts an username as input \n",
    "\n",
    "    # counter variable is used to skip first row which contains the headers\n",
    "    counter = 0\n",
    "    for row in csvreader:\n",
    "        if counter > 0:\n",
    "            usernames.append(row[1])\n",
    "            comments.append(row[3])\n",
    "        counter += 1\n",
    "        \n",
    "    # check for number of usernames and comments\n",
    "    if (len(usernames) != len(comments)):\n",
    "        print ('Unequal number of entries in Author and Content... Aborting...')\n",
    "        sys.exit()\n",
    "\n",
    "    # pass the required username and the comments for that username to reducer stage\n",
    "    for x in range(len(usernames)):\n",
    "        if required_username == usernames[x]:\n",
    "            print (str(usernames[x]) + '\\t' + str(comments[x]))\n",
    "            \n",
    "\n",
    "\n",
    "# read in through standard input\n",
    "lines = sys.stdin.readlines()\n",
    "\n",
    "# csvreader is a reader object which will iterate over lines\n",
    "csvreader = csv.reader(lines)\n",
    "\n",
    "# lists to store usernames and comments\n",
    "usernames = []\n",
    "comments = []\n",
    "\n",
    "\n",
    "# get username from command line argument\n",
    "\n",
    "required_username = str(sys.argv[1])\n",
    "\n",
    "# Run the mapper function for that username\n",
    "\n",
    "mapper_function(required_username)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/env python\n",
    "#Answer for reducer.py\n",
    "\n",
    "import sys\n",
    "\n",
    "final_dict = {\n",
    "    'username': None,\n",
    "    'comments': []\n",
    "}\n",
    "\n",
    "# get input from mapper job\n",
    "\n",
    "input_pairs = sys.stdin.readlines()\n",
    "\n",
    "\n",
    "\n",
    "for input_pair in input_pairs:\n",
    "    # split the input by '\\t' to separate out the key and the value\n",
    "    # the second param to split specifies that the maximum number of splits is 1\n",
    "    input_list = input_pair.split('\\t', 1)\n",
    "    if (len(input_list) != 2):\n",
    "        continue\n",
    "        \n",
    "    # append each comment\n",
    "    final_dict['comments'].append(input_list[1])\n",
    "    # set the username if it is not set\n",
    "    if final_dict['username'] is None:\n",
    "        final_dict['username'] = input_list[0]\n",
    "    \n",
    "\n",
    "# print out the output in desired form: username\\t[..comments..]\n",
    "print (final_dict.values()[0] + '\\t' + str(final_dict.values()[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run the hadoop commands for the mapreduce\n",
    "\n",
    "The directory structure is:\n",
    "```\n",
    "-Notebooks\n",
    "    -\\output2\n",
    "    -mapper2.py\n",
    "    -reducer2.py\n",
    "    -Youtube01-Psy.csv\n",
    "    -Youtube02-KatyPerry.csv\n",
    "    -Youtube03-LMFAO.csv\n",
    "    -Youtube04-Eminem.csv\n",
    "    -Youtube05-Shakira.csv\n",
    "    \n",
    "```\n",
    "\n",
    "We pass the argument of the username as shown below within \"\" so that the space separated string is treated \n",
    "as a single argument for our mapper file.\n",
    "\n",
    "In the example below we have passed in the user: \"Kiddy Kidso\" as the argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop switched to standalone mode.\n",
      "packageJobJar: [./mapper2.py] [] /tmp/streamjob3696086078029075728.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenJDK Server VM warning: You have loaded library /opt/hadoop-2.8.5/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.\n",
      "It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'.\n",
      "18/12/16 02:57:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "18/12/16 02:57:55 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "18/12/16 02:57:55 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "18/12/16 02:57:55 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "18/12/16 02:57:55 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "18/12/16 02:57:55 INFO mapred.FileInputFormat: Total input files to process : 5\n",
      "18/12/16 02:57:56 INFO mapreduce.JobSubmitter: number of splits:5\n",
      "18/12/16 02:57:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1804969977_0001\n",
      "18/12/16 02:57:56 INFO mapred.LocalDistributedCacheManager: Localized file:/home/comp6235/Notebooks/CW2/mapper2.py as file:/tmp/hadoop-comp6235/mapred/local/1544929076494/mapper2.py\n",
      "18/12/16 02:57:56 INFO mapred.LocalDistributedCacheManager: Localized file:/home/comp6235/Notebooks/CW2/reducer2.py as file:/tmp/hadoop-comp6235/mapred/local/1544929076495/reducer2.py\n",
      "18/12/16 02:57:57 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "18/12/16 02:57:57 INFO mapreduce.Job: Running job: job_local1804969977_0001\n",
      "18/12/16 02:57:57 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "18/12/16 02:57:57 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "18/12/16 02:57:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/16 02:57:57 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/16 02:57:57 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "18/12/16 02:57:57 INFO mapred.LocalJobRunner: Starting task: attempt_local1804969977_0001_m_000000_0\n",
      "18/12/16 02:57:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/16 02:57:57 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/16 02:57:57 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: Processing split: file:/home/comp6235/Notebooks/CW2/Youtube04-Eminem.csv:0+82896\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: numReduceTasks: 1\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: soft limit at 83886080\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "18/12/16 02:57:57 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/CW2/./mapper2.py, Kiddy Kidso]\n",
      "18/12/16 02:57:57 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "18/12/16 02:57:57 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "18/12/16 02:57:57 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "18/12/16 02:57:57 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "18/12/16 02:57:57 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "18/12/16 02:57:57 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "18/12/16 02:57:57 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "18/12/16 02:57:57 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "18/12/16 02:57:57 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "18/12/16 02:57:57 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "18/12/16 02:57:57 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "18/12/16 02:57:57 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "18/12/16 02:57:57 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/16 02:57:57 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/16 02:57:57 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/16 02:57:57 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/12/16 02:57:57 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/12/16 02:57:57 INFO mapred.LocalJobRunner: \n",
      "18/12/16 02:57:57 INFO mapred.MapTask: Starting flush of map output\n",
      "18/12/16 02:57:57 INFO mapred.Task: Task:attempt_local1804969977_0001_m_000000_0 is done. And is in the process of committing\n",
      "18/12/16 02:57:57 INFO mapred.LocalJobRunner: file:/home/comp6235/Notebooks/CW2/Youtube04-Eminem.csv:0+82896\n",
      "18/12/16 02:57:57 INFO mapred.Task: Task 'attempt_local1804969977_0001_m_000000_0' done.\n",
      "18/12/16 02:57:57 INFO mapred.Task: Final Counters for attempt_local1804969977_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=86321\n",
      "\t\tFILE: Number of bytes written=386604\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=454\n",
      "\t\tMap output records=0\n",
      "\t\tMap output bytes=0\n",
      "\t\tMap output materialized bytes=6\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=26\n",
      "\t\tTotal committed heap usage (bytes)=137433088\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=82896\n",
      "18/12/16 02:57:57 INFO mapred.LocalJobRunner: Finishing task: attempt_local1804969977_0001_m_000000_0\n",
      "18/12/16 02:57:57 INFO mapred.LocalJobRunner: Starting task: attempt_local1804969977_0001_m_000001_0\n",
      "18/12/16 02:57:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/16 02:57:57 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/16 02:57:57 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: Processing split: file:/home/comp6235/Notebooks/CW2/Youtube05-Shakira.csv:0+72706\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: numReduceTasks: 1\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: soft limit at 83886080\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "18/12/16 02:57:57 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/CW2/./mapper2.py, Kiddy Kidso]\n",
      "18/12/16 02:57:57 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/16 02:57:57 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/16 02:57:57 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/16 02:57:57 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/12/16 02:57:57 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/12/16 02:57:57 INFO mapred.LocalJobRunner: \n",
      "18/12/16 02:57:57 INFO mapred.MapTask: Starting flush of map output\n",
      "18/12/16 02:57:57 INFO mapred.Task: Task:attempt_local1804969977_0001_m_000001_0 is done. And is in the process of committing\n",
      "18/12/16 02:57:57 INFO mapred.LocalJobRunner: file:/home/comp6235/Notebooks/CW2/Youtube05-Shakira.csv:0+72706\n",
      "18/12/16 02:57:57 INFO mapred.Task: Task 'attempt_local1804969977_0001_m_000001_0' done.\n",
      "18/12/16 02:57:57 INFO mapred.Task: Final Counters for attempt_local1804969977_0001_m_000001_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=159580\n",
      "\t\tFILE: Number of bytes written=386642\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=371\n",
      "\t\tMap output records=0\n",
      "\t\tMap output bytes=0\n",
      "\t\tMap output materialized bytes=6\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=22\n",
      "\t\tTotal committed heap usage (bytes)=184619008\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=72706\n",
      "18/12/16 02:57:57 INFO mapred.LocalJobRunner: Finishing task: attempt_local1804969977_0001_m_000001_0\n",
      "18/12/16 02:57:57 INFO mapred.LocalJobRunner: Starting task: attempt_local1804969977_0001_m_000002_0\n",
      "18/12/16 02:57:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/16 02:57:57 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/16 02:57:57 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: Processing split: file:/home/comp6235/Notebooks/CW2/Youtube03-LMFAO.csv:0+64419\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: numReduceTasks: 1\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: soft limit at 83886080\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "18/12/16 02:57:57 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "18/12/16 02:57:57 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/CW2/./mapper2.py, Kiddy Kidso]\n",
      "18/12/16 02:57:57 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/16 02:57:57 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/16 02:57:57 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/16 02:57:57 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/12/16 02:57:57 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/12/16 02:57:57 INFO mapred.LocalJobRunner: \n",
      "18/12/16 02:57:57 INFO mapred.MapTask: Starting flush of map output\n",
      "18/12/16 02:57:58 INFO mapred.Task: Task:attempt_local1804969977_0001_m_000002_0 is done. And is in the process of committing\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: file:/home/comp6235/Notebooks/CW2/Youtube03-LMFAO.csv:0+64419\n",
      "18/12/16 02:57:58 INFO mapred.Task: Task 'attempt_local1804969977_0001_m_000002_0' done.\n",
      "18/12/16 02:57:58 INFO mapred.Task: Final Counters for attempt_local1804969977_0001_m_000002_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=224552\n",
      "\t\tFILE: Number of bytes written=386680\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=439\n",
      "\t\tMap output records=0\n",
      "\t\tMap output bytes=0\n",
      "\t\tMap output materialized bytes=6\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=22\n",
      "\t\tTotal committed heap usage (bytes)=169480192\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=64419\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: Finishing task: attempt_local1804969977_0001_m_000002_0\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: Starting task: attempt_local1804969977_0001_m_000003_0\n",
      "18/12/16 02:57:58 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/16 02:57:58 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/16 02:57:58 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: Processing split: file:/home/comp6235/Notebooks/CW2/Youtube02-KatyPerry.csv:0+64279\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: numReduceTasks: 1\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: soft limit at 83886080\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "18/12/16 02:57:58 INFO mapreduce.Job: Job job_local1804969977_0001 running in uber mode : false\n",
      "18/12/16 02:57:58 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/12/16 02:57:58 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/CW2/./mapper2.py, Kiddy Kidso]\n",
      "18/12/16 02:57:58 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/16 02:57:58 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/16 02:57:58 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/16 02:57:58 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/12/16 02:57:58 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: \n",
      "18/12/16 02:57:58 INFO mapred.MapTask: Starting flush of map output\n",
      "18/12/16 02:57:58 INFO mapred.Task: Task:attempt_local1804969977_0001_m_000003_0 is done. And is in the process of committing\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: file:/home/comp6235/Notebooks/CW2/Youtube02-KatyPerry.csv:0+64279\n",
      "18/12/16 02:57:58 INFO mapred.Task: Task 'attempt_local1804969977_0001_m_000003_0' done.\n",
      "18/12/16 02:57:58 INFO mapred.Task: Final Counters for attempt_local1804969977_0001_m_000003_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=289384\n",
      "\t\tFILE: Number of bytes written=386718\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=351\n",
      "\t\tMap output records=0\n",
      "\t\tMap output bytes=0\n",
      "\t\tMap output materialized bytes=6\n",
      "\t\tInput split bytes=109\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=25\n",
      "\t\tTotal committed heap usage (bytes)=219594752\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=64279\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: Finishing task: attempt_local1804969977_0001_m_000003_0\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: Starting task: attempt_local1804969977_0001_m_000004_0\n",
      "18/12/16 02:57:58 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/16 02:57:58 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/16 02:57:58 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: Processing split: file:/home/comp6235/Notebooks/CW2/Youtube01-Psy.csv:0+57438\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: numReduceTasks: 1\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: soft limit at 83886080\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "18/12/16 02:57:58 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/CW2/./mapper2.py, Kiddy Kidso]\n",
      "18/12/16 02:57:58 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/16 02:57:58 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/16 02:57:58 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/16 02:57:58 INFO streaming.PipeMapRed: Records R/W=351/1\n",
      "18/12/16 02:57:58 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/12/16 02:57:58 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: \n",
      "18/12/16 02:57:58 INFO mapred.MapTask: Starting flush of map output\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: Spilling map output\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: bufstart = 0; bufend = 77; bufvoid = 104857600\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
      "18/12/16 02:57:58 INFO mapred.MapTask: Finished spill 0\n",
      "18/12/16 02:57:58 INFO mapred.Task: Task:attempt_local1804969977_0001_m_000004_0 is done. And is in the process of committing\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: Records R/W=351/1\n",
      "18/12/16 02:57:58 INFO mapred.Task: Task 'attempt_local1804969977_0001_m_000004_0' done.\n",
      "18/12/16 02:57:58 INFO mapred.Task: Final Counters for attempt_local1804969977_0001_m_000004_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=347375\n",
      "\t\tFILE: Number of bytes written=386835\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=351\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=77\n",
      "\t\tMap output materialized bytes=85\n",
      "\t\tInput split bytes=103\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=23\n",
      "\t\tTotal committed heap usage (bytes)=137437184\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=57438\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: Finishing task: attempt_local1804969977_0001_m_000004_0\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: Starting task: attempt_local1804969977_0001_r_000000_0\n",
      "18/12/16 02:57:58 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/12/16 02:57:58 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/12/16 02:57:58 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/12/16 02:57:58 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@10bb430\n",
      "18/12/16 02:57:58 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "18/12/16 02:57:58 INFO reduce.EventFetcher: attempt_local1804969977_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "18/12/16 02:57:58 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1804969977_0001_m_000002_0 decomp: 2 len: 6 to MEMORY\n",
      "18/12/16 02:57:58 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1804969977_0001_m_000002_0\n",
      "18/12/16 02:57:58 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
      "18/12/16 02:57:58 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1804969977_0001_m_000004_0 decomp: 81 len: 85 to MEMORY\n",
      "18/12/16 02:57:58 INFO reduce.InMemoryMapOutput: Read 81 bytes from map-output for attempt_local1804969977_0001_m_000004_0\n",
      "18/12/16 02:57:58 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 81, inMemoryMapOutputs.size() -> 2, commitMemory -> 2, usedMemory ->83\n",
      "18/12/16 02:57:58 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1804969977_0001_m_000001_0 decomp: 2 len: 6 to MEMORY\n",
      "18/12/16 02:57:58 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1804969977_0001_m_000001_0\n",
      "18/12/16 02:57:58 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 3, commitMemory -> 83, usedMemory ->85\n",
      "18/12/16 02:57:58 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1804969977_0001_m_000003_0 decomp: 2 len: 6 to MEMORY\n",
      "18/12/16 02:57:58 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1804969977_0001_m_000003_0\n",
      "18/12/16 02:57:58 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 4, commitMemory -> 85, usedMemory ->87\n",
      "18/12/16 02:57:58 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1804969977_0001_m_000000_0 decomp: 2 len: 6 to MEMORY\n",
      "18/12/16 02:57:58 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1804969977_0001_m_000000_0\n",
      "18/12/16 02:57:58 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 5, commitMemory -> 87, usedMemory ->89\n",
      "18/12/16 02:57:58 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: 5 / 5 copied.\n",
      "18/12/16 02:57:58 INFO reduce.MergeManagerImpl: finalMerge called with 5 in-memory map-outputs and 0 on-disk map-outputs\n",
      "18/12/16 02:57:58 INFO mapred.Merger: Merging 5 sorted segments\n",
      "18/12/16 02:57:58 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 67 bytes\n",
      "18/12/16 02:57:58 INFO reduce.MergeManagerImpl: Merged 5 segments, 89 bytes to disk to satisfy reduce memory limit\n",
      "18/12/16 02:57:58 INFO reduce.MergeManagerImpl: Merging 1 files, 85 bytes from disk\n",
      "18/12/16 02:57:58 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "18/12/16 02:57:58 INFO mapred.Merger: Merging 1 sorted segments\n",
      "18/12/16 02:57:58 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 67 bytes\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: 5 / 5 copied.\n",
      "18/12/16 02:57:58 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/CW2/././reducer2.py]\n",
      "18/12/16 02:57:58 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "18/12/16 02:57:58 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "18/12/16 02:57:58 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/12/16 02:57:58 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "18/12/16 02:57:58 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/12/16 02:57:58 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/12/16 02:57:58 INFO mapred.Task: Task:attempt_local1804969977_0001_r_000000_0 is done. And is in the process of committing\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: 5 / 5 copied.\n",
      "18/12/16 02:57:58 INFO mapred.Task: Task attempt_local1804969977_0001_r_000000_0 is allowed to commit now\n",
      "18/12/16 02:57:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1804969977_0001_r_000000_0' to file:/home/comp6235/Notebooks/CW2/output2/_temporary/0/task_local1804969977_0001_r_000000\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "18/12/16 02:57:58 INFO mapred.Task: Task 'attempt_local1804969977_0001_r_000000_0' done.\n",
      "18/12/16 02:57:58 INFO mapred.Task: Final Counters for attempt_local1804969977_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=347729\n",
      "\t\tFILE: Number of bytes written=387024\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=109\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=1\n",
      "\t\tShuffled Maps =5\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=5\n",
      "\t\tGC time elapsed (ms)=2\n",
      "\t\tTotal committed heap usage (bytes)=137437184\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=104\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: Finishing task: attempt_local1804969977_0001_r_000000_0\n",
      "18/12/16 02:57:58 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "18/12/16 02:57:59 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/12/16 02:57:59 INFO mapreduce.Job: Job job_local1804969977_0001 completed successfully\n",
      "18/12/16 02:57:59 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1454941\n",
      "\t\tFILE: Number of bytes written=2320503\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1966\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=77\n",
      "\t\tMap output materialized bytes=109\n",
      "\t\tInput split bytes=530\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=109\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=2\n",
      "\t\tShuffled Maps =5\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=5\n",
      "\t\tGC time elapsed (ms)=120\n",
      "\t\tTotal committed heap usage (bytes)=986001408\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=341738\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=104\n",
      "18/12/16 02:57:59 INFO streaming.StreamJob: Output directory: output2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Clear output\n",
    "rm -rf output2\n",
    "\n",
    "# Make sure hadoop is in standalone mode\n",
    "hadoop-standalone-mode.sh\n",
    "\n",
    "# Main pipeline command\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "-files mapper2.py,reducer2.py \\\n",
    "-input ./Youtube01-Psy.csv ./Youtube02-KatyPerry.csv ./Youtube03-LMFAO.csv ./Youtube04-Eminem.csv ./Youtube05-Shakira.csv \\\n",
    "-mapper 'mapper2.py \"Kiddy Kidso\"' -file ./mapper2.py  \\\n",
    "-reducer ./reducer2.py \\\n",
    "-output output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the above step runs we get the output in the directory: **output2** and in the file: **part-00000**\n",
    "\n",
    "The output is of the format:\n",
    "\n",
    "```\n",
    "Username\t[\"Comment 1\", \"Comment 2\", \"Comment 3\", \"etc.\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expected key-value output format:\n",
    "#John Smith\t[\"Comment 1\", \"Comment 2\", \"Comment 3\", \"etc.\"]\n",
    "#Jane Doe\t[\"Comment 1\", \"Comment 2\", \"Comment 3\", \"etc.\"]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
