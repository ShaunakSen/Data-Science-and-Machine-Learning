{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Words and Sentences with NLTK\n",
    "\n",
    "\n",
    "[link](https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', ',', 'all', 'work', 'and', 'no', 'play']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "data = \"All work and no play makes jack a dull boy, all work and no play\"\n",
    "print(word_tokenize(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All work and no play makes jack dull boy.', 'All work and no play makes jack a dull boy.']\n"
     ]
    }
   ],
   "source": [
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "\n",
    "print (sent_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All work and no play makes jack dull boy.', 'All work and no play makes jack a dull boy.']\n",
      "['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'dull', 'boy', '.', 'All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', '.']\n"
     ]
    }
   ],
   "source": [
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    " \n",
    "phrases = sent_tokenize(data)\n",
    "words = word_tokenize(data)\n",
    " \n",
    "print(phrases)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing: remove stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['work',\n",
       " 'play',\n",
       " 'makes',\n",
       " 'jack',\n",
       " 'dull',\n",
       " 'boy',\n",
       " '.',\n",
       " 'work',\n",
       " 'play',\n",
       " 'makes',\n",
       " 'jack',\n",
       " 'dull',\n",
       " 'boy',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "wordsFiltered = []\n",
    "for word in words:\n",
    "    if word.strip().lower() not in stopWords:\n",
    "        wordsFiltered.append(word.strip().lower())\n",
    "        \n",
    "wordsFiltered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK – stemming\n",
    "\n",
    "A word stem is part of a word. It is sort of a normalization idea, but linguistic.\n",
    "For example, the stem of the word waiting is wait.\n",
    "\n",
    "![](https://pythonspot-9329.kxcdn.com/wp-content/uploads/2016/08/word-stem.png.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game\n",
      "game\n",
      "game\n",
      "game\n",
      "work\n",
      "work\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "words = [\"game\",\"gaming\",\"gamed\",\"games\", \"working\", \"worked\", \"works\"]\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print (ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaming:game\n",
      ",:,\n",
      "the:the\n",
      "gamers:gamer\n",
      "play:play\n",
      "games:game\n"
     ]
    }
   ],
   "source": [
    "sentence = \"gaming, the gamers play games\"\n",
    "words = word_tokenize(sentence)\n",
    " \n",
    "for word in words:\n",
    "    print(word + \":\" + ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Cleaning with NLTK\n",
    "\n",
    "[link](https://machinelearningmastery.com/clean-text-machine-learning-python/)\n",
    "\n",
    "### Split into sentences\n",
    "\n",
    "A good useful first step is to split the text into sentences.\n",
    "\n",
    "Some modeling tasks prefer input to be in the form of paragraphs or sentences, such as word2vec. You could first split your text into sentences, split each sentence into words, then save each sentence to file, one per line.\n",
    "\n",
    "NLTK provides the sent_tokenize() function to split text into sentences.\n",
    "\n",
    "The example below loads the “metamorphosis_clean.txt” file into memory, splits it into sentences, and prints the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./metamorphosis_clean.txt', mode='rt')\n",
    "text = file.read()\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One morning, when Gregor Samsa woke from troubled dreams, he found\\nhimself transformed in his bed into a horrible vermin.', 'He lay on\\nhis armour-like back, and if he lifted his head a little he could\\nsee his brown belly, slightly domed and divided by arches into stiff\\nsections.', 'The bedding was hardly able to cover it and seemed ready\\nto slide off any moment.', 'His many legs, pitifully thin compared\\nwith the size of the rest of him, waved about helplessly as he\\nlooked.', '\"What\\'s happened to me?\"']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print (sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, we can see that although the document is split into sentences, that each sentence still preserves the new line from the artificial wrap of the lines in the original document.\n",
    "\n",
    "### Split into words\n",
    "\n",
    "NLTK provides a function called word_tokenize() for splitting strings into tokens (nominally words).\n",
    "\n",
    "It splits tokens based on white space and punctuation. For example, commas and periods are taken as separate tokens. Contractions are split apart (e.g. “What’s” becomes “What” “‘s“). Quotes are kept, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', '.', 'He', 'lay', 'on', 'his', 'armour-like', 'back', ',', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', ',', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', '.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', '.', 'His', 'many', 'legs', ',', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', '.', '``', 'What', \"'s\", 'happened', 'to']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print (tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can filter out all tokens that we are not interested in, such as all standalone punctuation.\n",
    "\n",
    "This can be done by iterating over all tokens and only keeping those tokens that are all alphabetic. Python has the function isalpha() that can be used. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'-' in 'sdsds-assa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour-like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 'happened', 'to', 'me', 'he', 'thought', 'It', 'was', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "words_filtered = []\n",
    "\n",
    "for word in tokens:\n",
    "    if word.isalpha() or '-' in word:\n",
    "        words_filtered.append(word)\n",
    "        \n",
    "print(words_filtered[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the example, you can see that not only punctuation tokens, but examples like “armour-like” and “‘s” were also filtered out.**\n",
    "\n",
    "### Filter out Stop Words (and Pipeline)\n",
    "\n",
    "Stop words are those words that do not contribute to the deeper meaning of the phrase.\n",
    "\n",
    "They are the most common words such as: “the“, “a“, and “is“.\n",
    "\n",
    "For some applications like documentation classification, it may make sense to remove stop words.\n",
    "\n",
    "NLTK provides a list of commonly agreed upon stop words for a variety of languages, such as English. They can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can see that they are all lower case and have punctuation removed.**\n",
    "\n",
    "You could compare your tokens to the stop words and filter them out, but you must ensure that your text is prepared the same way.\n",
    "\n",
    "Let’s demonstrate this with a small pipeline of text preparation including:\n",
    "\n",
    "\n",
    "- Load the raw text.\n",
    "- Split into tokens.\n",
    "- Convert to lowercase.\n",
    "- Remove punctuation from each token.\n",
    "- Filter out remaining tokens that are not alphabetic.\n",
    "- Filter out tokens that are stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "words_filtered_final = []\n",
    "\n",
    "for word in words_filtered:\n",
    "    if word not in stop_words:\n",
    "        words_filtered_final.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem words\n",
    "\n",
    "Stemming refers to the process of reducing each word to its root or base.\n",
    "\n",
    "For example “fishing,” “fished,” “fisher” all reduce to the stem “fish.”\n",
    "\n",
    "Some applications, like document classification, may benefit from stemming in order to both reduce the vocabulary and to focus on the sense or sentiment of a document rather than deeper meaning.\n",
    "\n",
    "There are many stemming algorithms, although a popular and long-standing method is the Porter Stemming algorithm. This method is available in NLTK via the PorterStemmer class.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morn', 'gregor', 'samsa', 'woke', 'troubl', 'dream', 'found', 'transform', 'bed', 'horribl', 'vermin', 'He', 'lay', 'armour-lik', 'back', 'lift', 'head', 'littl', 'could', 'see', 'brown', 'belli', 'slightli', 'dome', 'divid', 'arch', 'stiff', 'section', 'the', 'bed', 'hardli', 'abl', 'cover', 'seem', 'readi', 'slide', 'moment', 'hi', 'mani', 'leg', 'piti', 'thin', 'compar', 'size', 'rest', 'wave', 'helplessli', 'look', 'what', 'happen', 'thought', 'It', 'dream', 'hi', 'room', 'proper', 'human', 'room', 'although', 'littl', 'small', 'lay', 'peac', 'four', 'familiar', 'wall', 'A', 'collect', 'textil', 'sampl', 'lay', 'spread', 'tabl', '-', 'samsa', 'travel', 'salesman', '-', 'hung', 'pictur', 'recent', 'cut', 'illustr', 'magazin', 'hous', 'nice', 'gild', 'frame', 'It', 'show', 'ladi', 'fit', 'fur', 'hat', 'fur', 'boa', 'sat', 'upright', 'rais']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "stemmed_words = []\n",
    "\n",
    "for word in words_filtered_final:\n",
    "    stemmed_words.append(porter.stem(word))\n",
    "    \n",
    "print(stemmed_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A pro tip is to continually review your tokens after every transform. I have tried to show that in this tutorial and I hope you take that to heart. Ideally, you would save a new file after each transform so that you can spend time with all of the data in the new form. Things always jump out at you when to take the time to review your data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alike', 'wish', 'like', 'the_like', 'similar', 'the_likes_of', 'same', 'corresponding', 'ilk', 'care', 'comparable'}\n",
      "{'dislike', 'unalike', 'unlike'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet \n",
    "synonyms = [] \n",
    "antonyms = [] \n",
    "  \n",
    "for syn in wordnet.synsets(\"like\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "print(set(synonyms)) \n",
    "print(set(antonyms)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
