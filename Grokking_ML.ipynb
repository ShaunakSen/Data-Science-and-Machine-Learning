{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grokking ML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/IGfVM8IjYocnRmfsSSwv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Data-Science-and-Machine-Learning/blob/master/Grokking_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELwT41IVwTIz"
      },
      "source": [
        "## Grokking ML interview\n",
        "\n",
        "> Based on the course by edicative.io: https://www.educative.io/courses/grokking-the-machine-learning-interview/\n",
        "\n",
        "---\n",
        "\n",
        "### What to expect in a machine learning interview?\n",
        "\n",
        "Companies hiring for machine learning roles conduct interviews to assess individual abilities in various areas. You can expect the following topics to be covered in these interviews:\n",
        "\n",
        "![](https://i.imgur.com/l1oW0M1.png)\n",
        "\n",
        "**Problem-solving/coding**\n",
        "\n",
        "This portion of the interview is fairly similar to other software engineering coding interviews where the interviewer gives a coding problem, such as perform an ‘In-order tree traversal’, and the candidate is expected to solve that in about half an hour. There is ample content available on how to best prepare for such questions.\n",
        "\n",
        "**Machine learning understanding**\n",
        "\n",
        "This area generally focuses on individual understanding of basic ML concepts such as supervised vs. unsupervised learning, reinforcement learning, classification vs. regression, deep learning, optimization functions, and the learning process of various ML algorithms. There are many courses and books that go over these fundamental concepts. They facilitate the learning of ML basics and help candidates prepare for the interview.\n",
        "\n",
        "**Career Discussions**\n",
        "\n",
        "Career discussion tends to focus on an individual’s resume (previous projects) and behavioral aspects, such as the ability to work in teams (conflict resolution) and career motivation. Understanding the path you want to take in your career and having the ability to discuss previous experiences and projects is required for this portion.\n",
        "\n",
        "**Machine learning system design discussion**\n",
        "\n",
        "This discussion focuses on the interviewee’s ability to solve an __end-to-end__ machine learning problem and consists of open-ended questions. This is an integral part of the interview, and not much helping material is available for it. Hence, this course helps in developing the thought pattern required to approach ML system design questions.\n",
        "\n",
        "In the ML system design interview portion, candidates are given open-ended machine learning problems and are expected to build an end-to-end machine learning system to solve that problem. Few of such problems could be:\n",
        "\n",
        "- Build a recommendation system that shows relevant products to users.\n",
        "- Build a visual understanding system for a self-driving car.\n",
        "- Build a search-ranking system.\n",
        "\n",
        "![](https://i.imgur.com/XEboyvw.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zahtZXI1p-l"
      },
      "source": [
        "### Setting up a Machine Learning System\n",
        "\n",
        "For instance, you may be asked to design a search engine that displays the most relevant results in response to user queries. You could narrow down the problem’s scope by asking the following questions:\n",
        "\n",
        "- Is it a general search engine like Google or Bing or a specialized search engine like Amazon’s products search?\n",
        "\n",
        "- What kind of queries is it expected to answer?\n",
        "\n",
        "This will allow you to precisely define your ML problem statement as follows:\n",
        "\n",
        "> Build a generic search engine that returns relevant results for queries like \"Richard Nixon\", \"Programming languages\" etc.\n",
        "\n",
        "Or, you may be asked to build a system to display a Twitter feed for a user. In this case, you can discuss how the feed is currently displayed and how it can be improved to provide a better experience for the users.\n",
        "\n",
        "After inspecting the problem from all aspects, you can easily narrow it down to a precise machine learning problem statement as follows:\n",
        "\n",
        "> \"Given a list of tweets, train an ML model that predicts the probability of engagement of tweets and orders them based on that score.\"\n",
        "\n",
        "NOTE: Some problems may require you to think about hardware components that could provide input for the machine learning models.\n",
        "\n",
        "**Understanding scale and latency requirements**\n",
        "\n",
        "Another very important part of the problem setup is the discussion about performance and capacity considerations of the system. This conversation will allow you to clearly understand the scale of the system and its requirements.\n",
        "\n",
        "Let’s look at some examples of the questions you need to ask.\n",
        "\n",
        "**Latency requirements**\n",
        "\n",
        "If you were given the search engine problem, you would ask:\n",
        "\n",
        "    Do we want to return the search result in 100 milliseconds or 500 milliseconds?\n",
        "\n",
        "Similarly, if you were given the Twitter feed problem, you would ask:\n",
        "\n",
        "    Do we want to return the list of relevant tweets in 300 milliseconds or 400 milliseconds?\n",
        "\n",
        "**Scale of the data**\n",
        "\n",
        "Again, for the search engine problem, you would ask:\n",
        "\n",
        "    How many requests per second do we anticipate to handle?\n",
        "\n",
        "\n",
        "    How many websites exist that we want to enable through this search engine?\n",
        "\n",
        "    If a query has 10 billion matching documents, how many of these would be ranked by our model?\n",
        "\n",
        "And, for the Twitter feed problem, you would ask:\n",
        "\n",
        "    How many tweets would we have to rank according to relevance for a user at a time?\n",
        "\n",
        "\n",
        "The answers to these questions will guide you when you come up with the architecture of the system. **Knowing that you need to return results quickly will influence the depth and complexity of your models. Having huge amounts of data to process, you will design the system with scalability in mind. Find more on this in the architecture discussion section.**\n",
        "\n",
        "**Defining metrics**\n",
        "\n",
        "Now that you have figured out what machine learning problem you want to solve, the next step is to come up with metrics. Metrics will help you to see if your system is performing well.\n",
        "\n",
        "Knowing our success criteria helps in understanding the problem and in selecting key architectural components. __This is why it’s important to discuss metrics early in our design discussions.__\n",
        "\n",
        "**Metrics for offline testing**\n",
        "\n",
        "You will use offline metrics to quickly test the models’ performance during the development phase. You may have generic metrics; for example, if you are performing binary classification, you will use AUC, log loss, precision, recall, and F1-score. In other cases, you might have to come up with specific metrics for a certain problem. For instance, for the search ranking problem, you would use __NDCG__ as a metric.\n",
        "\n",
        "> What is NDCG: https://github.com/ShaunakSen/Data-Science-and-Machine-Learning/blob/master/ML_Concepts.ipynb\n",
        "\n",
        "**Metrics for online testing**\n",
        "\n",
        "Once you have selected the best performing models offline, you will use online metrics to test them in the production environment. The decision to deploy the newly created model depends on its performance in an online test.\n",
        "\n",
        "While coming up with online metrics, you may need both component-wise and end-to-end metrics. Consider that you are making a search ranking model to display relevant results for search queries. You may use a component-wise metric such as NDCG to measure the performance of your model online. However, you also need to look at how the system (search engine) is performing with your new model plugged in, for which you can use end-to-end metrics. A commonly used end-to-end metric for this scenario is the users’ engagement and retention rate.\n",
        "\n",
        "In another scenario, you may be asked to develop the ML system for a task that may be used as a component in other tasks. Again, you need both component level metrics and end-to-end metrics during online testing. For instance, you could be asked to design the ML system for *entity linking* which is going to be used to improve search relevance.\n",
        "\n",
        "Here, you will have component-wise metrics to evaluate the performance of the entity linking model individually. You will also be expected to come up with metrics for the search engine where the entity linking component will ultimately be plugged in.\n",
        "\n",
        "For example in the color matching work, an example of component metric will be accuracy while an end-to-end metric will be the overall dashboard performance\n",
        "\n",
        "#### Architecture discussion\n",
        "\n",
        "The next step is to figure out the architecture of the system. You need to think about the components of the system and how the data will flow through those components.\n",
        "\n",
        "To get an idea, have a peek at how the architecture of a search engine’s ML system may be designed, below.\n",
        "\n",
        "![](https://i.imgur.com/SSFrFex.png)\n",
        "\n",
        "You start off with a searcher making a query on the search engine. Let’s take an example query: “itlian restaurant”. The query may be poorly worded or misspelled so you need the query rewriting component to rewrite it. It would correct the query to “italian restaurant”. This allows you to retrieve better results. Moving forward, the query understanding component helps identify the intent of a query. The “italian restaurant” query has local intent. Next, the document selection component selects relevant documents from the billions of documents on the web. The ranker component ranks them according to relevance. Finally, the results are displayed on the search engine result page (SERP).\n",
        "\n",
        "As we mentioned previously, the requirements gathered during problem setup help you in chalking out the architecture. For instance, you are tasked with building an ML system that displays relevant ads to users. During its problem setup, you ask questions and realize that the number of users and ads in the system is huge and ever-increasing. Thus, you need a scalable system that quickly figures out the relevant ads for all users despite the increase in data.\n",
        "\n",
        "### Offline model building and evaluation\n",
        "\n",
        "The next step is to start building the model offline and then evaluate it. This step involves:\n",
        "\n",
        "1. Training data generation:\n",
        "\n",
        "Broadly speaking, we have two methods of training data collection/generation for supervised learning tasks: \n",
        "\n",
        "- Human labeled data (This is an expensive way to gather data. So we need to supplement it with in-house labelers or open-source datasets.), \n",
        "\n",
        "- Data collection through a user’s interaction with the pre-existing system.\n",
        "Another way to gather data is through the online (currently deployed/pre-existing) system. Online system refers to a running system, currently in place, that people are interacting with. For example, a running search engine or ad system that people are using. To get an idea, let’s go back to the search ranking example, where it was asked to design a new ML-based system to show relevant search results. The currently deployed version of the search engine would also show results for user queries (using an existing ML-based system or a rule-based system). You can see how people interact with these results to gather training data. If a user clicks on a result (link), you can count it as a positive training example. Similarly, an impression can count as a negative example\n",
        "\n",
        "2. Feature engineering\n",
        "\n",
        "This is another very crucial step as good features influence the model’s ability to a great extent. You start this process by explicitly pinpointing the __actors__ involved in the given task, which you have implicitly identified during problem setup as well.\n",
        "\n",
        "For example, if the task at hand is to make movie recommendations to a Netflix user, you would have the following actors: \n",
        "\n",
        "![](https://i.imgur.com/0oJ8fAY.png)\n",
        "\n",
        "In order to make features, you would individually inspect these actors and explore their relationships too. For instance, if you individually inspect the logged-in user, you can come up with features such as the user’s age, gender, language, etc. Likewise, if you look at the context, an important feature could be the “upcoming holiday”. If Christmas is approaching and the movie under consideration is also a Christmas movie, there is a greater chance that the user would watch it. Similarly, we can look at the historical engagement between the user and media to come up with features. An example could be the user’s interaction with the movie’s genre in the last three months.\n",
        "\n",
        "![](https://i.imgur.com/ZcVZPc7.png)\n",
        "\n",
        "3. Model training:\n",
        "\n",
        "Now, you can finally decide on the ML models that you should use for the given tasks, keeping the performance and capacity considerations in mind. We can also try out different hyperparameter values to see what works best.\n",
        "\n",
        "If you are using the funnel approach, you may select simpler models for the top of the funnel where data size is huge and more complex neural networks or trees based models for successive parts of the funnel. (Kind of counter-intuitive as more complex models generally perform more with more data). Suppose the best thing while deciding on a funnel approach will be the latency requirements. If minimum latency is reqd we need simpler models \n",
        "\n",
        "We also have the option of utilizing pre-trained SOTA (state of the art) models to leverage the power of transfer learning (you don’t need to reinvent the wheel completely each time).\n",
        "\n",
        "4. Offline evaluation:\n",
        "\n",
        "With careful consideration, divide the data into training and validation sets. Use the training data set during the model training part, where you come up with various modeling options (different models, hyperparameters, feature sets, etc.). Afterwards, we evaluate these models, offline, on the validation dataset. Utilize the metrics you decided on earlier, to measure their performance. The top few models, showing the most promise, are taken to the next stage.\n",
        "\n",
        "### Online model execution and evaluation\n",
        "\n",
        "Now that you have selected the top-performing models, you will test them in an online environment. Online testing heavily influences the decision of deploying the model. This is where online metrics come into play.\n",
        "\n",
        "Depending on the type of problem, you may use both component level and end-to-end metrics. As mentioned before, for the search engine task, the component-wise metric will be NDCG in online testing. However, this alone is not enough. You also need an end-to-end metric as well, like session success rate, to see if the system’s (search engine’s) performance has increased by using your new search ranking ML model.\n",
        "\n",
        "If you see a substantial increase in system performance during the online test, you can deploy it on production.\n",
        "\n",
        "### Iterative model improvement \n",
        "\n",
        "Your model may perform well during offline testing, but the same increase in performance may not be observed during an online test. Here, you need to think about *debugging the model* to find out what exactly is causing this behavior.\n",
        "\n",
        "Is a particular component not working correctly? Is the features’ distribution different during training and testing time? For instance, a feature called “user’s top five interest” may show a difference in distribution during training and testing, when plotted. This can help you to identify that the routines used to provide the top five user interests were significantly different during training and testing time.\n",
        "\n",
        "Moreover, after the first version of your model has been built and deployed, you still need to monitor its performance. If the model is not performing as expected, you need to go towards debugging. You may observe a general failure from a decrease in AUC. Or, you may note that the model is failing in particular scenarios. For instance, by analysing the video recording of the self-driving car, you may find out that the image segmentation fails in rushy areas.\n",
        "\n",
        "The problem areas identified during model debugging will guide you in building successive iterations of your model.\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPqb_fkSwPlU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}