{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn Scrapy\n",
    "\n",
    "### Getting Started with Web Scraping using Scrapy\n",
    "\n",
    "\n",
    "[YouTube link](https://www.youtube.com/watch?v=vkA1cWN4DEc&list=PLZyvi_9gamL-EE3zQJbU5N3nzJcfNeFHU)\n",
    "\n",
    "`scrapy shell http://quotes.toscrape.com/random`\n",
    "\n",
    "This downloads the website for us to use\n",
    "\n",
    "`print (response.text)`\n",
    "\n",
    "### We can use CSS selectors\n",
    "\n",
    "```python\n",
    "In [2]: response.css('small.author')\n",
    "Out[2]: [<Selector xpath=\"descendant-or-self::small[@class and contains(concat(' ', normalize-space(@class), ' '), ' author ')]\" data='<small class=\"author\" itemprop=\"author\">'>]\n",
    "\n",
    "In [3]: response.css('small.author').extract()\n",
    "Out[3]: ['<small class=\"author\" itemprop=\"author\">George R.R. Martin</small>']\n",
    "\n",
    "In [4]: response.css('small.author::text').extract()\n",
    "Out[4]: ['George R.R. Martin']\n",
    "\n",
    "In [5]: response.css('small.author::text').extract()[0]\n",
    "Out[5]: 'George R.R. Martin'\n",
    "\n",
    "In [6]: response.css('small.author::text').extract_first()\n",
    "Out[6]: 'George R.R. Martin'\n",
    "\n",
    "```\n",
    "\n",
    "### Getting quote and tag text\n",
    "\n",
    "```python\n",
    "\n",
    "In [7]: response.css('span.text::text').extract_first()\n",
    "Out[7]: '“A reader lives a thousand lives before he dies, said Jojen. The man who never reads lives only one.”'\n",
    "    \n",
    "In [1]: response.css('a.tag::text').extract()\n",
    "Out[1]: ['books', 'inspirational', 'reading', 'tea']\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### Creating a spider\n",
    "\n",
    "`scrapy genspider quotes http://toscrape.com/\n",
    "`\n",
    "\n",
    "quotes is the name of our spider, toscrape.com is the domain of the website that we want to scape\n",
    "\n",
    "``` python\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['toscrape.com/']\n",
    "    # urls that we want our spider to visit\n",
    "    start_urls = ['http://quotes.toscrape.com/random', 'http://quotes.toscrape.com/random']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # callback method\n",
    "        self.log(\"Visited..\" + response.url)\n",
    "        yield {\n",
    "            'author_name': response.css('small.author::text').extract_first(),\n",
    "            'text': response.css('span.text::text').extract_first(),\n",
    "            'tags': response.css('a.tag::text').extract()\n",
    "        }\n",
    "```\n",
    "\n",
    "```\n",
    "scrapy runspider quotes.py -o items.json\n",
    "\n",
    "Run the spider and save the op to the JSON file\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Multiple items from a page\n",
    "\n",
    "[this page]() has many quotes: 10\n",
    "\n",
    "We want to scrape thes 10 quotes\n",
    "\n",
    "If we run `response.css('a.tag::text').extract()` it will return all tags in the page\n",
    "\n",
    "But we want tag by quote\n",
    "\n",
    "So we have to extract each quote one by one\n",
    "\n",
    "```python\n",
    "\n",
    "def parse(self, response):\n",
    "        # callback method\n",
    "        quotes = response.css('div.quote')\n",
    "        for quote in quotes:\n",
    "            item = {\n",
    "                'author_name': quote.css('small.author::text').extract_first(),\n",
    "                'text': quote.css('span.text::text').extract_first(),\n",
    "                'tags': quote.css('a.tag::text').extract()\n",
    "            }\n",
    "            yield item\n",
    "        \n",
    "```\n",
    "\n",
    "### Pagination Links \n",
    "\n",
    "```python\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['toscrape.com']\n",
    "    # urls that we want our spider to visit\n",
    "    start_urls = ['http://quotes.toscrape.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # callback method\n",
    "        quotes = response.css('div.quote')\n",
    "        for quote in quotes:\n",
    "            item = {\n",
    "                'author_name': quote.css('small.author::text').extract_first(),\n",
    "                'text': quote.css('span.text::text').extract_first(),\n",
    "                'tags': quote.css('a.tag::text').extract()\n",
    "            }\n",
    "            yield item\n",
    "\n",
    "        next_page_url = response.css('li.next > a::attr(href)').extract_first()\n",
    "        next_page_url = response.urljoin(next_page_url)\n",
    "        print (\"Next url:\", next_page_url)    \n",
    "        if next_page_url:\n",
    "            # create new request\n",
    "            print (\"Here\")\n",
    "            yield scrapy.Request(url = next_page_url, callback=self.parse)    \n",
    "        else:\n",
    "            print (\"Next url is \", next_page_url)    \n",
    "            \n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "{\n",
    " 'downloader/request_count': 10,\n",
    " 'item_scraped_count': 100\n",
    "}\n",
    "```\n",
    "\n",
    "### Getting Author Details\n",
    "\n",
    "If we are already in the scrapy shell for a particular page, we can use the `fetch()` command to get resoinse from another page\n",
    "\n",
    "We use a diff callback to handle the authors page requests\n",
    "\n",
    "``` python\n",
    "\n",
    "class AuthorsspiderSpider(scrapy.Spider):\n",
    "    name = 'AuthorsSpider'\n",
    "    allowed_domains = ['toscrape.com']\n",
    "    start_urls = ['http://quotes.toscrape.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        author_urls = response.css('div.quote > span > a::attr(href)').extract()\n",
    "         \n",
    "        for author_url in author_urls:\n",
    "            url = response.urljoin(author_url)\n",
    "            # create a request to the url with a separate callback\n",
    "            print (\"Author URL...\", url)\n",
    "            yield scrapy.Request(url = url, callback=self.parse_details)\n",
    "        next_page_url = response.css('li.next > a::attr(href)').extract_first()\n",
    "        next_page_url = response.urljoin(next_page_url)\n",
    "        print (\"Next url:\", next_page_url)    \n",
    "        if next_page_url:\n",
    "            yield scrapy.Request(url = next_page_url, callback=self.parse)    \n",
    "\n",
    "    def parse_details(self, response):\n",
    "        # extract data from author\n",
    "        yield {\n",
    "            'name':response.css('h3.author-title::text').extract_first().strip(),\n",
    "            'birthdate': response.css('span.author-born-date::text').extract_first().strip()\n",
    "        }\n",
    "                    \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
