{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn Scrapy\n",
    "\n",
    "### Getting Started with Web Scraping using Scrapy\n",
    "\n",
    "\n",
    "[YouTube link](https://www.youtube.com/watch?v=vkA1cWN4DEc&list=PLZyvi_9gamL-EE3zQJbU5N3nzJcfNeFHU)\n",
    "\n",
    "`scrapy shell http://quotes.toscrape.com/random`\n",
    "\n",
    "This downloads the website for us to use\n",
    "\n",
    "`print (response.text)`\n",
    "\n",
    "### We can use CSS selectors\n",
    "\n",
    "```python\n",
    "In [2]: response.css('small.author')\n",
    "Out[2]: [<Selector xpath=\"descendant-or-self::small[@class and contains(concat(' ', normalize-space(@class), ' '), ' author ')]\" data='<small class=\"author\" itemprop=\"author\">'>]\n",
    "\n",
    "In [3]: response.css('small.author').extract()\n",
    "Out[3]: ['<small class=\"author\" itemprop=\"author\">George R.R. Martin</small>']\n",
    "\n",
    "In [4]: response.css('small.author::text').extract()\n",
    "Out[4]: ['George R.R. Martin']\n",
    "\n",
    "In [5]: response.css('small.author::text').extract()[0]\n",
    "Out[5]: 'George R.R. Martin'\n",
    "\n",
    "In [6]: response.css('small.author::text').extract_first()\n",
    "Out[6]: 'George R.R. Martin'\n",
    "\n",
    "```\n",
    "\n",
    "### Getting quote and tag text\n",
    "\n",
    "```python\n",
    "\n",
    "In [7]: response.css('span.text::text').extract_first()\n",
    "Out[7]: '“A reader lives a thousand lives before he dies, said Jojen. The man who never reads lives only one.”'\n",
    "    \n",
    "In [1]: response.css('a.tag::text').extract()\n",
    "Out[1]: ['books', 'inspirational', 'reading', 'tea']\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### Creating a spider\n",
    "\n",
    "`scrapy genspider quotes http://toscrape.com/\n",
    "`\n",
    "\n",
    "quotes is the name of our spider, toscrape.com is the domain of the website that we want to scape\n",
    "\n",
    "``` python\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['toscrape.com/']\n",
    "    # urls that we want our spider to visit\n",
    "    start_urls = ['http://quotes.toscrape.com/random', 'http://quotes.toscrape.com/random']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # callback method\n",
    "        self.log(\"Visited..\" + response.url)\n",
    "        yield {\n",
    "            'author_name': response.css('small.author::text').extract_first(),\n",
    "            'text': response.css('span.text::text').extract_first(),\n",
    "            'tags': response.css('a.tag::text').extract()\n",
    "        }\n",
    "```\n",
    "\n",
    "```\n",
    "scrapy runspider quotes.py -o items.json\n",
    "\n",
    "Run the spider and save the op to the JSON file\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Multiple items from a page\n",
    "\n",
    "[this page]() has many quotes: 10\n",
    "\n",
    "We want to scrape thes 10 quotes\n",
    "\n",
    "If we run `response.css('a.tag::text').extract()` it will return all tags in the page\n",
    "\n",
    "But we want tag by quote\n",
    "\n",
    "So we have to extract each quote one by one\n",
    "\n",
    "```python\n",
    "\n",
    "def parse(self, response):\n",
    "        # callback method\n",
    "        quotes = response.css('div.quote')\n",
    "        for quote in quotes:\n",
    "            item = {\n",
    "                'author_name': quote.css('small.author::text').extract_first(),\n",
    "                'text': quote.css('span.text::text').extract_first(),\n",
    "                'tags': quote.css('a.tag::text').extract()\n",
    "            }\n",
    "            yield item\n",
    "        \n",
    "```\n",
    "\n",
    "### Pagination Links \n",
    "\n",
    "```python\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['toscrape.com']\n",
    "    # urls that we want our spider to visit\n",
    "    start_urls = ['http://quotes.toscrape.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # callback method\n",
    "        quotes = response.css('div.quote')\n",
    "        for quote in quotes:\n",
    "            item = {\n",
    "                'author_name': quote.css('small.author::text').extract_first(),\n",
    "                'text': quote.css('span.text::text').extract_first(),\n",
    "                'tags': quote.css('a.tag::text').extract()\n",
    "            }\n",
    "            yield item\n",
    "\n",
    "        next_page_url = response.css('li.next > a::attr(href)').extract_first()\n",
    "        next_page_url = response.urljoin(next_page_url)\n",
    "        print (\"Next url:\", next_page_url)    \n",
    "        if next_page_url:\n",
    "            # create new request\n",
    "            print (\"Here\")\n",
    "            yield scrapy.Request(url = next_page_url, callback=self.parse)    \n",
    "        else:\n",
    "            print (\"Next url is \", next_page_url)    \n",
    "            \n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "{\n",
    " 'downloader/request_count': 10,\n",
    " 'item_scraped_count': 100\n",
    "}\n",
    "```\n",
    "\n",
    "### Getting Author Details\n",
    "\n",
    "If we are already in the scrapy shell for a particular page, we can use the `fetch()` command to get resoinse from another page\n",
    "\n",
    "We use a diff callback to handle the authors page requests\n",
    "\n",
    "``` python\n",
    "\n",
    "class AuthorsspiderSpider(scrapy.Spider):\n",
    "    name = 'AuthorsSpider'\n",
    "    allowed_domains = ['toscrape.com']\n",
    "    start_urls = ['http://quotes.toscrape.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        author_urls = response.css('div.quote > span > a::attr(href)').extract()\n",
    "         \n",
    "        for author_url in author_urls:\n",
    "            url = response.urljoin(author_url)\n",
    "            # create a request to the url with a separate callback\n",
    "            print (\"Author URL...\", url)\n",
    "            yield scrapy.Request(url = url, callback=self.parse_details)\n",
    "        next_page_url = response.css('li.next > a::attr(href)').extract_first()\n",
    "        next_page_url = response.urljoin(next_page_url)\n",
    "        print (\"Next url:\", next_page_url)    \n",
    "        if next_page_url:\n",
    "            yield scrapy.Request(url = next_page_url, callback=self.parse)    \n",
    "\n",
    "    def parse_details(self, response):\n",
    "        # extract data from author\n",
    "        yield {\n",
    "            'name':response.css('h3.author-title::text').extract_first().strip(),\n",
    "            'birthdate': response.css('span.author-born-date::text').extract_first().strip()\n",
    "        }\n",
    "                    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infinite Scrolling Pages\n",
    "\n",
    "For infinite scrolling pages we can see the rquests being made to the server from the Network tab of the dev tools\n",
    "\n",
    "![](./img/diag1.png)\n",
    "\n",
    "Also we can see the actual data received from the server in the Network -> Preview tab\n",
    "\n",
    "This data is already structured in JSON format\n",
    "\n",
    "We can easily parse the response.text as a dictionary in python\n",
    "\n",
    "```python\n",
    "\n",
    "scrapy shell http://quotes.toscrape.com/scroll\n",
    "\n",
    "import json\n",
    "\n",
    "data = json.loads(response.text)\n",
    "\n",
    "print(data.keys())\n",
    "\n",
    "```\n",
    "\n",
    "`Out[12]: dict_keys(['has_next', 'page', 'quotes', 'tag', 'top_ten_tags'])`\n",
    "\n",
    "hes_next tells us when we have to stop making requests\n",
    "\n",
    "page tells us which page we are currently on\n",
    "\n",
    "**Thus for any web scraping on a field, including infiiinite scrolling, look for the reqs the browser is making under the Network tab**\n",
    "\n",
    "This will help us fing the underlying api, which we can then use\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "class QuotesscrollSpider(scrapy.Spider):\n",
    "    name = 'QuotesScroll'\n",
    "    allowed_domains = ['toscrape.com']\n",
    "    api_url = 'http://quotes.toscrape.com/api/quotes?page={}'\n",
    "    start_urls = [api_url.format(1)]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # parse the response data\n",
    "\n",
    "        data = json.loads(response.text)\n",
    "\n",
    "        for quote in data['quotes']:\n",
    "            yield {\n",
    "                'author_name': quote['author']['name'],\n",
    "                'text': quote['text'],\n",
    "                'tags': quote['tags']\n",
    "            }\n",
    "        # check if next_page available\n",
    "\n",
    "        if data['has_next']:\n",
    "            next_page = data['page'] + 1\n",
    "            # generate new req for next page\n",
    "            yield scrapy.Request(url = self.api_url.format(next_page), callback=self.parse)\n",
    "                \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POST reqs using scrapy\n",
    "\n",
    "In the quotes website, the author's goodreads link is only visible if we are logged in\n",
    "\n",
    "Our spider will\n",
    "\n",
    "1. Login to the site with username and pwd\n",
    "\n",
    "2. Scrape info\n",
    "\n",
    "As before go into the network tab in the login page and see the reqs being made\n",
    "\n",
    "We see that the browser made a `POST` req to url: `http://quotes.toscrape.com/login`\n",
    "\n",
    "The request url is not an API, it is simply the login page\n",
    "\n",
    "Also scrolling down we can see from the Form data that the browser sent 3 params to the server\n",
    "\n",
    "- csrf_token\n",
    "\n",
    "- username\n",
    "\n",
    "- password\n",
    "\n",
    "On insoecting the Login page we find that along with username and pwd field there is a hidden input for csrf \n",
    "\n",
    "Inspect , delete the csrf token field and submit the for. It throws an error message\n",
    "\n",
    "Also, everytime the page is reloaded, we get a new token\n",
    "\n",
    "Method:\n",
    "\n",
    "1. Spider downloads login pg\n",
    "2. Extracts the csrf data from the page and at it to the form data that we want to submit\n",
    "3. It will create a req to the action url from the form \n",
    "4. After logging in, it will get the author name and the goodreads url\n",
    "\n",
    "We see that the csrf token is like:\n",
    "\n",
    "```html\n",
    "<input type=\"hidden\" name=\"csrf_token\" value=\"qFTafnKpvDlCGAMjytSQbNeZiVOmgPUkJhIYLrwHEuXcRzsWBoxd\">\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "In [2]: response.css('input[name = \"csrf_token\"]::attr(value)').extract_first()\n",
    "Out[2]: 'okaYKGwTDHJvryUlcINeWzfFChxMbgVOmqBnLZQSPsARjtdEuiXp'\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "def parse(self, response):\n",
    "        # get csrf token\n",
    "        token = response.css('input[name = \"csrf_token\"]::attr(value)').exract_first()\n",
    "        print (\"CSRF TOKEN IS...\", token)\n",
    "        # create dict with data we want to send to server\n",
    "\n",
    "        data = {\n",
    "            'csrf_token': token,\n",
    "            'username': 'shaunak1105',\n",
    "            'password': 'abcd'\n",
    "        }\n",
    "\n",
    "        # submit a POST request\n",
    "\n",
    "        yield scrapy.FormRequest(url = self.login_url, formdata = self.data, callback = self.pase_quotes())\n",
    "\n",
    "```\n",
    "\n",
    "Next we write the code to extract author info\n",
    "\n",
    "[css selectors](https://www.w3schools.com/csSref/css_selectors.asp)\n",
    "[xpath](http://www.zvon.org/comp/r/tut-XPath_1.html#Pages~Start_with_%2F%2F)\n",
    "\n",
    "```python\n",
    "\n",
    "def parse_quotes(self, response):\n",
    "        # parse the page after spider is logged in\n",
    "\n",
    "        # for each quote\n",
    "            # extract the goodreads link of the author\n",
    "\n",
    "            for quote in response.css(\"div.quote\"):\n",
    "                yield {\n",
    "                    'author_name': quote.css(\"small.author::text\").exract_first(),\n",
    "                    'author_url': quote.css('small.author ~ a[href*=\"goodreads.com\"]::attr(href)').\n",
    "    extract_first()\n",
    "                }\n",
    "                \n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
