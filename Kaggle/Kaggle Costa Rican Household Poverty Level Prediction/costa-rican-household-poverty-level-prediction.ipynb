{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Costa Rican Household Poverty Level Prediction\n\n> Competition Link: https://www.kaggle.com/c/costa-rican-household-poverty-prediction\n\n> All credits go to the amazing notebook by Will Koehrsen: https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough\n\n> Will Koehrsen Kaggle profile: https://www.kaggle.com/willkoehrsen  | Medium: http://medium.com/@williamkoehrsen/\n\n---\n\n\nWelcome to another Kaggle challenge! The objective of the Costa Rican Household Poverty Level Prediction contest is to develop a machine learning model that can predict the poverty level of households using both individual and household characteristics. This \"data science for good\" project offers the opportunity to put our skills towards a task more beneficial to society than getting people to click on ads!\n\nIn this notebook, we will walk through a complete machine learning solution: first, get introduced to the problem, then perform a thorough Exploratory Data Analysis of the dataset, work on feature engineering, try out multiple machine learning models, select a model, work to optimize the model, and finally, inspect the outputs of the model and draw conclusions. __While this notebook may not get us to the top of the leaderboard, it is meant to be used as a teaching tool to give you a solid foundation to build on for future machine learning projects. Kaggle projects can teach us a lot about machine learning, but several of the strategies used to get to the very top of the leaderboard are not best practices, so here we'll stick to building a very good - although not quite first place - machine learning solution. While Kaggle projects are competitions, I think they are best described as \"a machine learning education\" disguised as a contest!__\"\n\n### Problem and Data Explanation\n\n\nThe data for this competition is provided in two files: `train.csv` and `test.csv`. \n\nhe training set has 9557 rows and 143 columns while the testing set has 23856 rows and 142 columns. Each row represents __one individual__ and each column is a __feature, either unique to the individual, or for the household of the individual__. \n\nThe training set has one additional column, `Target`, which represents the poverty level on a 1-4 scale and is the label for the competition. A value of 1 is the most extreme poverty. \n\nThis is a __supervised multi-class classification machine learning problem__\n\n### Objective\n\nThe objective is to predict poverty on a __household level__. We are given data on the individual level with each individual having unique features but also information about their household. In order to create a dataset for the task, we'll have to perform some _aggregations of the individual data_ for each household. Moreover, we have to make a prediction for every individual in the test set, but _\"ONLY the heads of household are used in scoring\"_ which means we want to predict poverty on a household basis. \n\n\n__Important note: while all members of a household should have the same label in the training data, there are errors where individuals in the same household have different labels. In these cases, we are told to use the label for the head of each household, which can be identified by the rows where `parentesco1 == 1.0`.__ We will cover how to correct this in the notebook\n\n\nThe `Target` values represent poverty levels as follows:\n\n    1 = extreme poverty \n    2 = moderate poverty \n    3 = vulnerable households \n    4 = non vulnerable households\n    \n    \nThe explanations for all 143 columns can be found in the [competition documentation](https://www.kaggle.com/c/costa-rican-household-poverty-prediction/data), but a few to note are below:\n\n* __Id__: a unique identifier for each individual, this should not be a feature that we use! \n* __idhogar__: a unique identifier for each household. This variable is not a feature, but will be used to group individuals by household as all individuals in a household will have the same identifier.\n* __parentesco1__: indicates if this person is the head of the household.\n* __Target__: the label, which should be equal for all members in a household\n\nWhen we make a model, we'll train on a household basis with the label for each household _the poverty level of the head of household_. The raw data contains a mix of both household and individual characteristics and for the individual data, we will have to find a way to aggregate this for each household. Some of the individuals belong to a household with _no head of household_ which means that unfortunately we can't use this data for training. These issues with the data are completely typical of __real-world__ data and hence this problem is great preparation for the datasets you'll encounter in a data science job! \n"},{"metadata":{},"cell_type":"markdown","source":"### Some ideas based on data description\n\nSome features have an order to them\n\n```\nepared1, =1 if walls are bad\nepared2, =1 if walls are regular\nepared3, =1 if walls are good\netecho1, =1 if roof are bad\netecho2, =1 if roof are regular\netecho3, =1 if roof are good\neviv1, =1 if floor are bad\neviv2, =1 if floor are regular\neviv3, =1 if floor are good\n```\n\nWe can encode them numerically\n\n---\n\n`hogar_total = hogar_adul + hogar_nin`: check if true, if yes, its a redundant feature\n\n---\n\nWhat info do these squared vars add?\n\n```\nSQBescolari, escolari squared\nSQBage, age squared\nSQBhogar_total, hogar_total squared\nSQBedjefe, edjefe squared\nSQBhogar_nin, hogar_nin squared\nSQBovercrowding, overcrowding squared\nSQBdependency, dependency squared\nSQBmeaned, square of the mean years of education of adults (>=18) in the household\nagesq, Age squared\n```"},{"metadata":{},"cell_type":"markdown","source":"### Metric\n\nUltimately we want to build a machine learning model that can predict the integer poverty level of a household. Our predictions will be assessed by the __Macro F1 Score.__ You may be familiar with the [standard F1 score](http://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/) for binary classification problems which is the harmonic mean of precision and recall:\n\n$$F_1 = \\frac{2}{\\tfrac{1}{\\mathrm{recall}} + \\tfrac{1}{\\mathrm{precision}}} = 2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}$$\n\nFor mutli-class problems, we have to average the F1 scores for each class. The macro F1 score averages the F1 score for each class _without taking into account label imbalances_. \n\n$$\\text{Macro F1} = \\frac{\\text{F1 Class 1} + \\text{F1 Class 2} + \\text{F1 Class 3} + \\text{F1 Class 4}}{4}$$\n\nIn other words, the number of occurrences of each label does not figure into the calculation when using macro (while it does when using the \"weighted\" score). (For more information on the differences, look at the [Scikit-Learn Documention for F1 Score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) or this [Stack Exchange question and answers](https://datascience.stackexchange.com/q/15989/42908). If we want to assess our performance, we can use the code:\n\n```\nfrom sklearn.metrics import f1_score\nf1_score(y_true, y_predicted, average = 'macro`)\n```\n\n__For this problem, the labels are imbalanced, which makes it a little strange to use macro averaging for the evaluation metric, but that's a decision made by the organizers and not something we can change! In your own work, you want to be aware of label imbalances and choose a metric accordingly.__\n\n### Roadmap\n\nThe end objective is a machine learning model that can predict the poverty level of a household. However, before we get carried away with modeling, it's important to understand the problem and data. Also, we want to evaluate numerous models before choosing one as the \"best\" and after building a model, we want to investigate the predictions. Our roadmap is therefore as follows:\n\n\n1. Understand the problem (we're almost there already)\n2. Exploratory Data Analysis\n3. Feature engineering to create a dataset for machine learning\n4. Compare several baseline machine learning models\n5. Try more complex machine learning models\n6. Optimize the selected model\n7. Investigate model predictions in context of problem\n6. Draw conclusions and lay out next steps \n\nThe steps laid out above are iterative meaning that while we will go through them one at a time, we might go back to an earlier step and revisit some of our decisions. In general, data science is a non-linear pracice where we are constantly evaluating our past decisions and making improvements. In particular, feature engineering, modeling, and optimization are steps that we often repeat because we never know if we got them right the first time!\n\nWe have a pretty good grasp of the problem, so we'll move into the Exploratory Data Analysis (EDA) and feature engineering. For the EDA we'll examine any interesting anomalies, trends, correlations, or patterns that can be used for feature engineering and for modeling. We'll make sure to investigate our data both quantitatively (with statistics) and visually (with figures). \n\nOnce we have a good grasp of the data and any potentially useful relationships, we can do some feature engineering (the most important part of the machine learning pipeline) and establish a baseline model. This won't get us to the top of the leaderboard, but it will provide a strong foundation to build on! \n\nWith all that info in mind (don't worry if you haven't got all the details), let's get started! \n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}